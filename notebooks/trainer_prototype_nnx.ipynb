{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1440ce-9dee-447c-93a5-65b8c7e889e3",
   "metadata": {},
   "source": [
    "# Trainer NNX scratchpad\n",
    "\n",
    "Generate training data\n",
    "```\n",
    "python rgi/main.py --game connect4 --player1 random --player2 random --num_games 100 --save_trajectories\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a19b2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "print(jax.devices())\n",
    "assert jax.devices()[0].platform == 'gpu'\n",
    "\n",
    "# print wider lines to stop arrays wrapping so soon. numpy default is 75.\n",
    "jnp.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e34972",
   "metadata": {},
   "source": [
    "# Load Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d5ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories_glob: ../data/trajectories/connect4/*.trajectory.npy\n",
      "num_trajectories: 100\n"
     ]
    }
   ],
   "source": [
    "from rgi.core import trajectory\n",
    "\n",
    "game_name = \"connect4\"\n",
    "trajectories_glob = os.path.join(\"..\", \"data\", \"trajectories\", game_name, \"*.trajectory.npy\")\n",
    "trajectories = trajectory.load_trajectories(trajectories_glob)\n",
    "\n",
    "print(f'trajectories_glob: {trajectories_glob}')\n",
    "print(f'num_trajectories: {len(trajectories)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322895c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 18:17:17.428828: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.77. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 1, win:  14, loss:   9, win_pct: 60.87%\n",
      "action: 2, win:   6, loss:   6, win_pct: 50.00%\n",
      "action: 3, win:   8, loss:   4, win_pct: 66.67%\n",
      "action: 4, win:  11, loss:   3, win_pct: 78.57%\n",
      "action: 5, win:  11, loss:   5, win_pct: 68.75%\n",
      "action: 6, win:   8, loss:   5, win_pct: 61.54%\n",
      "action: 7, win:   5, loss:   5, win_pct: 50.00%\n"
     ]
    }
   ],
   "source": [
    "def print_trajectory_stats():\n",
    "    c = Counter((t.actions[0].item(), t.final_rewards[0].item()) for t in trajectories)\n",
    "    for action in range(1,7+1):\n",
    "        win_count, loss_count = c[(action, 1.0)], c[(action, -1.0)]\n",
    "        print(f'action: {action}, win: {win_count:3d}, loss: {loss_count:3d}, win_pct: {win_count / (win_count + loss_count) * 100:.2f}%')\n",
    "\n",
    "print_trajectory_stats()\n",
    "\n",
    "# num_trajectories: 100\n",
    "# action: 1, win:  14, loss:   9, win_pct: 60.87%\n",
    "# action: 2, win:   6, loss:   6, win_pct: 50.00%\n",
    "# action: 3, win:   8, loss:   4, win_pct: 66.67%\n",
    "# action: 4, win:  11, loss:   3, win_pct: 78.57%\n",
    "# action: 5, win:  11, loss:   5, win_pct: 68.75%\n",
    "# action: 6, win:   8, loss:   5, win_pct: 61.54%\n",
    "# action: 7, win:   5, loss:   5, win_pct: 50.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5872e996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trajectory_steps: 2193\n"
     ]
    }
   ],
   "source": [
    "from rgi.core.trajectory import EncodedTrajectory\n",
    "\n",
    "# define trajectory NamedTuple\n",
    "from typing import NamedTuple\n",
    "class TrajectoryStep(NamedTuple):\n",
    "    move_index: int\n",
    "    state: jax.Array\n",
    "    action: jax.Array\n",
    "    next_state: jax.Array\n",
    "    reward: jax.Array\n",
    "\n",
    "\n",
    "# TODO: hack fixup to make reward (0,1) instead of (-1,1)\n",
    "def fixup_reward(x): return (x+1) / 2\n",
    "\n",
    "def unroll_trajectory(encoded_trajectories: list[EncodedTrajectory]):\n",
    "    for t in encoded_trajectories:\n",
    "        for i in range(t.length - 1):\n",
    "            yield TrajectoryStep(i, t.states[i], t.actions[i], t.states[i + 1], fixup_reward(t.final_rewards[0]))\n",
    "\n",
    "all_trajecoty_steps = list(unroll_trajectory(trajectories))\n",
    "print(f'num_trajectory_steps: {len(all_trajecoty_steps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c8bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch = jnp.array([t.state for t in all_trajecoty_steps])\n",
    "action_batch = jnp.array([t.action for t in all_trajecoty_steps])\n",
    "reward_batch = jnp.array([t.reward for t in all_trajecoty_steps])\n",
    "batch_full = {'state': state_batch, 'action': action_batch, 'reward': reward_batch}\n",
    "\n",
    "# batch with 1 or 2 steps for testing\n",
    "batch_1 = {'state': state_batch[:1], 'action': action_batch[:1], 'reward': reward_batch[:1]}\n",
    "batch_2 = {'state': state_batch[:2], 'action': action_batch[:2], 'reward': reward_batch[:2]}\n",
    "\n",
    "# Super easy to get 100% accuracy.\n",
    "batch_repeated = {'state': jnp.tile(batch_2['state'], (1000,1)), 'action': jnp.tile(batch_2['action'], (1000,)), 'reward': jnp.tile(batch_2['reward'], (1000,))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce65ffe",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36570865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.games import connect4\n",
    "game = connect4.Connect4Game()\n",
    "serializer = connect4.Connect4Serializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44221c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4StateEmbedder(nnx.Module):\n",
    "    embedding_dim: int = 64\n",
    "    hidden_dim: int = 256\n",
    "\n",
    "    def _state_to_array(self, encoded_state_batch: jax.Array) -> jax.Array:\n",
    "        board_array = encoded_state_batch[:, :-1].reshape([-1, 6, 7])\n",
    "        return board_array\n",
    "\n",
    "    def __init__(self, *, rngs: nnx.Rngs):\n",
    "        self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs)\n",
    "        self.linear1 = nnx.Linear(6*7*64, self.hidden_dim, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(self.hidden_dim, self.embedding_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, encoded_state_batch: jax.Array):\n",
    "        x = self._state_to_array(encoded_state_batch)\n",
    "        x = x[..., None]  # Add channel dimension\n",
    "        x = nnx.relu(self.conv1(x))\n",
    "        x = nnx.relu(self.conv2(x))\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten while preserving batch dimension\n",
    "        x = nnx.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b36c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.        ]\n",
      " [-0.03769782 -0.0449753   0.03002147  0.05141831 -0.01900685]]\n"
     ]
    }
   ],
   "source": [
    "def test_state_embedder():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(0))\n",
    "    print(state_embedder(trajectories[0].states[:2])[:,:5])\n",
    "\n",
    "test_state_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b11059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4ActionEmbedder(nnx.Module):\n",
    "    embedding_dim: int = 64\n",
    "    num_actions: int = 7\n",
    "\n",
    "    def __init__(self, *, rngs: nnx.Rngs):\n",
    "        self.embedding = nnx.Embed(num_embeddings=self.num_actions, features=self.embedding_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, action: jax.Array) -> jax.Array:\n",
    "        # Ensure action is 0-indexed\n",
    "        action = action - 1\n",
    "        return self.embedding(action)\n",
    "    \n",
    "    def all_action_embeddings(self):\n",
    "        # return self(jnp.arange(1,self.num_actions+1))\n",
    "        return self.embedding.embedding.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51cf2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.66427687e-02  1.09368816e-01 -1.89152256e-01  8.17345604e-02 -7.85972252e-02 -1.82855129e-01  1.57606214e-01  1.16470948e-01 -7.91573077e-02\n",
      "   1.19286872e-01 -3.06055341e-02 -2.94262134e-02 -2.04068035e-01  2.47045040e-01 -2.14104000e-02 -1.46370962e-01  3.72717567e-02  6.29766062e-02\n",
      "  -4.52014022e-02 -2.30931133e-01  1.21301249e-01  2.88546652e-01 -3.97264175e-02 -1.19131254e-02  1.30750760e-01  1.29211560e-01  3.51449512e-02\n",
      "   1.26254661e-02  7.07970336e-02  1.36475870e-02  3.25598754e-02  5.28224790e-03 -7.92904049e-02  2.16124147e-01  1.23112321e-01 -5.16295396e-02\n",
      "  -5.90516999e-03 -7.26998299e-02  1.06619947e-01  4.65621985e-02  1.09553948e-01 -3.07245195e-01  6.44185543e-02 -5.80371059e-02  1.00223973e-01\n",
      "  -4.86722216e-02  2.83067394e-02  2.01851264e-01  3.05441283e-02 -9.47859734e-02 -8.36363509e-02 -1.35540087e-02 -5.33435494e-02  7.62056708e-02\n",
      "   8.15421045e-02  1.95111567e-03 -1.68878466e-01 -1.05658814e-01 -2.38340721e-02  6.14980869e-02  7.95577615e-02 -1.57072678e-01  1.64067205e-05\n",
      "   8.12526867e-02]]\n",
      "[[ 0.15725625 -0.16321151  0.00577787 -0.13705109  0.04763619]\n",
      " [-0.10096847  0.0897982   0.17255177  0.04128496  0.2138293 ]\n",
      " [ 0.01521776 -0.06541049  0.02257618 -0.23189619  0.18868035]\n",
      " [ 0.15271325 -0.25990918 -0.13089019  0.00996069  0.17892274]\n",
      " [ 0.07929838  0.02261104 -0.15678175  0.15027006 -0.07462826]\n",
      " [ 0.06664277  0.10936882 -0.18915226  0.08173456 -0.07859723]\n",
      " [ 0.08239102 -0.19150715 -0.06263629 -0.09114807 -0.19348153]]\n"
     ]
    }
   ],
   "source": [
    "def test_action_embedder():\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(0))\n",
    "    print(action_embedder(trajectories[0].actions[:1]))\n",
    "    print(action_embedder.all_action_embeddings()[:,:5])    \n",
    "\n",
    "test_action_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b1d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel(nnx.Module):\n",
    "    \"\"\"Model to predict action probabilities and reward of the current state.\"\"\"\n",
    "    state_embedder: Connect4StateEmbedder\n",
    "    action_embedder: Connect4ActionEmbedder\n",
    "\n",
    "    embedding_dim: int = 64\n",
    "    num_actions: int = 7\n",
    "\n",
    "    def __init__(self, state_embedder: Connect4StateEmbedder, action_embedder: Connect4ActionEmbedder, *, rngs: nnx.Rngs):\n",
    "        self.state_embedder = state_embedder\n",
    "        self.action_embedder = action_embedder\n",
    "        self.reward_head = nnx.Linear(self.embedding_dim, 1, rngs=rngs)\n",
    "\n",
    "    def action_logits(self, state_batch: jax.Array) -> jax.Array:\n",
    "        state_embeddings = self.state_embedder(state_batch)\n",
    "        all_action_embeddings = self.action_embedder.all_action_embeddings()\n",
    "        logits = state_embeddings @ all_action_embeddings.T\n",
    "        return logits\n",
    "    \n",
    "    def action_probs(self, state_batch: jax.Array) -> jax.Array:\n",
    "        logits = self.action_logits(state_batch)\n",
    "        return jax.nn.softmax(logits)\n",
    "    \n",
    "    def reward_pred(self, state_batch: jax.Array) -> jax.Array:\n",
    "        state_embeddings = self.state_embedder(state_batch)\n",
    "        return self.reward_head(state_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cba82e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_logits:\n",
      " [[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.05365274  0.04191583 -0.05556455 -0.0902497  -0.04111161  0.00694049  0.01304063]]\n",
      "\n",
      "action_probs:\n",
      " [[0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715]\n",
      " [0.13876376 0.15267958 0.13849871 0.13377723 0.14051497 0.14743187 0.14833395]]\n",
      "\n",
      "reward_pred:\n",
      " [[ 0.        ]\n",
      " [-0.06306502]]\n"
     ]
    }
   ],
   "source": [
    "def test_state_action_model():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(0))\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(1))\n",
    "    state_action_model = PredictionModel(state_embedder, action_embedder, rngs=nnx.Rngs(2))\n",
    "    print('action_logits:\\n', state_action_model.action_logits(state_batch[:2]))\n",
    "    print('\\naction_probs:\\n', state_action_model.action_probs(state_batch[:2]))\n",
    "    print('\\nreward_pred:\\n', state_action_model.reward_pred(state_batch[:2]))\n",
    "\n",
    "test_state_action_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7637b",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a8d5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "@nnx.jit\n",
    "def l2_loss(model, alpha=0.001):\n",
    "    loss = sum(\n",
    "        (alpha * (w ** 2).sum()) \n",
    "        for w in jax.tree.leaves(nnx.state(model, nnx.Param))\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def loss_fn(prediction_model: PredictionModel, batch, l2_weight: float = 1e-4):\n",
    "    action_logits = prediction_model.action_logits(batch['state'])\n",
    "    action_labels = batch['action']-1\n",
    "    action_data_loss = optax.softmax_cross_entropy_with_integer_labels(logits=action_logits, labels=action_labels).mean()\n",
    "\n",
    "    reward_pred = prediction_model.reward_pred(batch['state'])\n",
    "    reward_labels = batch['reward']\n",
    "    reward_data_loss = ((reward_labels - reward_pred.squeeze())**2).mean()\n",
    "\n",
    "    parameter_loss_l2 = l2_weight * l2_loss(prediction_model)\n",
    "    \n",
    "    total_loss = action_data_loss + reward_data_loss + parameter_loss_l2\n",
    "    return total_loss, (action_logits, reward_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ed3a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.9921958\n",
      "action_logits:  [[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.05365274  0.04191583 -0.05556455 -0.0902497  -0.04111161  0.00694049  0.01304063]]\n",
      "reward_pred:  [[ 0.        ]\n",
      " [-0.06306502]]\n"
     ]
    }
   ],
   "source": [
    "def test_loss_fn():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(0))\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(1))\n",
    "    prediction_model = PredictionModel(state_embedder, action_embedder, rngs=nnx.Rngs(2))\n",
    "    loss, (action_logits, reward_pred) = loss_fn(prediction_model, batch_2)\n",
    "    print('loss: ', loss)\n",
    "    print('action_logits: ', action_logits)\n",
    "    print('reward_pred: ', reward_pred)\n",
    "\n",
    "with jax.disable_jit():\n",
    "    test_loss_fn()\n",
    "\n",
    "\n",
    "# loss:  1.927142\n",
    "# logits:  [[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
    "#  [-0.05365274  0.04191583 -0.05556455 -0.0902497  -0.04111161  0.00694049  0.01304064]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae318f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(prediction_model: PredictionModel, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, (logits, reward_pred)), grads = grad_fn(prediction_model, batch)\n",
    "  metrics.update(\n",
    "    loss=loss,\n",
    "    action_logits=logits, action_labels=batch['action']-1,\n",
    "    reward_pred=reward_pred.squeeze(), reward_labels=batch['reward'])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(prediction_model: PredictionModel, metrics: nnx.MultiMetric, batch):\n",
    "  loss, (logits, reward_pred) = loss_fn(prediction_model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch['action'])  # In-place updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a788f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "from abc import ABC, abstractmethod\n",
    "from typing_extensions import override\n",
    "\n",
    "class TwoValueAverageMetric(nnx.Metric, ABC):\n",
    "    total: nnx.metrics.MetricState\n",
    "    count: nnx.metrics.MetricState\n",
    "\n",
    "    def __init__(self, argname_1: str, argname_2: str):\n",
    "        self.argname_1 = argname_1\n",
    "        self.argname_2 = argname_2\n",
    "        self.total = nnx.metrics.MetricState(jnp.array(0, dtype=jnp.float32))\n",
    "        self.count = nnx.metrics.MetricState(jnp.array(0, dtype=jnp.int32))\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.total.value = jnp.array(0, dtype=jnp.float32)\n",
    "        self.count.value = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    def update(self, **kwargs) -> None:\n",
    "        if self.argname_1 not in kwargs:\n",
    "            raise TypeError(f\"Expected keyword argument '{self.argname_1}'\")\n",
    "        if self.argname_2 not in kwargs:\n",
    "            raise TypeError(f\"Expected keyword argument '{self.argname_2}'\")      \n",
    "        v1, v2 = kwargs[self.argname_1], kwargs[self.argname_2]\n",
    "        self.update_pair(v1, v2)\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_pair(v1: jax.Array, v2: jax.Array) -> None:\n",
    "        raise NotImplementedError(\"Must be implemented by subclass\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute(self) -> jax.Array:\n",
    "        raise NotImplementedError(\"Must be implemented by subclass\")\n",
    "\n",
    "class MeanSquaredError(TwoValueAverageMetric):\n",
    "    @override\n",
    "    def __init__(self, argname_1:str='mse_1', argname_2:str='mse_2'):\n",
    "        super().__init__(argname_1, argname_2)\n",
    "\n",
    "    @override\n",
    "    def update_pair(self, v1: jax.Array, v2: jax.Array) -> None:\n",
    "        if v1.shape != v2.shape:\n",
    "            raise ValueError(f\"Expected shapes {v1.shape} and {v2.shape} to be equal\")\n",
    "        self.total.value += ((v1 - v2)**2).sum()\n",
    "        self.count.value += v1.size\n",
    "\n",
    "    def compute(self) -> jax.Array:\n",
    "        \"\"\"Compute and return the average.\"\"\"\n",
    "        return self.total.value / self.count.value\n",
    "\n",
    "class LogitAccuracy(TwoValueAverageMetric):\n",
    "    @override\n",
    "    def __init__(self, argname_1:str='logits', argname_2:str='labels'):\n",
    "        super().__init__(argname_1, argname_2)\n",
    "\n",
    "    @override\n",
    "    def update_pair(self, logits: jax.Array, labels: jax.Array) -> None:\n",
    "        if logits.ndim != labels.ndim + 1:\n",
    "            raise ValueError(f'Expected logits.ndim==labels.ndim+1, got {logits.ndim} and {labels.ndim}')\n",
    "\n",
    "        self.total.value += (logits.argmax(axis=-1) == labels).sum()\n",
    "        self.count.value += labels.size\n",
    "\n",
    "    def compute(self) -> jax.Array:\n",
    "        \"\"\"Compute and return the average.\"\"\"\n",
    "        return self.total.value / self.count.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ba96d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3333334\n"
     ]
    }
   ],
   "source": [
    "def test_mean_squared_error():\n",
    "    mse = MeanSquaredError('reward_pred', 'reward_labels')\n",
    "    mse.update(reward_pred=jnp.array([1, 2, 5]), reward_labels=jnp.array([1, 2, 3]))\n",
    "    print(mse.compute())\n",
    "    mse.reset()\n",
    "\n",
    "test_mean_squared_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07c2b31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': Array(0.6666667, dtype=float32),\n",
      " 'action_accuracy': Array(0.6666667, dtype=float32),\n",
      " 'loss': Array(1., dtype=float32),\n",
      " 'reward_mse': Array(1.3333334, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def test_multi_metric():\n",
    "    metrics = nnx.MultiMetric(\n",
    "        accuracy=nnx.metrics.Accuracy(),\n",
    "        action_accuracy=LogitAccuracy(\"action_logits\",\"action_labels\"),\n",
    "        reward_mse=MeanSquaredError('reward_pred', 'reward_labels'),\n",
    "        loss=nnx.metrics.Average('loss'))\n",
    "    # metrics = nnx.MultiMetric(action=nnx.metrics.Accuracy(), loss=nnx.metrics.Average('loss'))\n",
    "    metrics.update(\n",
    "        loss=1.0,\n",
    "        logits=jnp.array([[1, 2, 3], [4, 5, 6], [3,1,1]]),\n",
    "        labels=jnp.array([1, 2, 0]),\n",
    "        action_logits=jnp.array([[1, 2, 3], [4, 5, 6], [3,1,1]]),\n",
    "        action_labels=jnp.array([1, 2, 0]),\n",
    "        reward_pred=jnp.array([1, 2, 5]),\n",
    "        reward_labels=jnp.array([1, 2, 3]),\n",
    "        )\n",
    "    pprint(metrics.compute())\n",
    "\n",
    "test_multi_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6230b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step: 1: {'action_accuracy': Array(0.14363885, dtype=float32), 'reward_mse': Array(1.029514, dtype=float32), 'loss': Array(3.0080228, dtype=float32)}\n",
      "0 r=1.0 p=0.010169084183871746 [[14.3043 14.3014 14.2703 14.2743 14.2685 14.2841 14.2971]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
      "1 r=1.0 p=0.24431966245174408 [[15.7219 14.4648 14.3187 13.4712 13.8857 13.7536 14.3841]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
      "2 r=1.0 p=0.39975491166114807 [[16.4178 15.4107 13.6721 13.142  14.1966 13.0371 14.1238]] Counter({4: 1, 3: 1, 1: 1})\n",
      "3 r=1.0 p=0.5405424237251282 [[16.6388 16.1784 13.4278 13.3675 14.019  12.1698 14.1989]] Counter({5: 1})\n",
      "\n",
      "train_step: 200: {'action_accuracy': Array(0.3995919, dtype=float32), 'reward_mse': Array(0.13890737, dtype=float32), 'loss': Array(1.7018539, dtype=float32)}\n",
      "0 r=1.0 p=0.6212881803512573 [[23.8711 10.9569 12.4906 14.7687 15.7316 12.7189  9.4622]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
      "1 r=1.0 p=0.551194965839386 [[ 9.6459  7.7541 24.6307  3.0861 10.2573 24.1751 20.4507]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
      "2 r=1.0 p=0.16818884015083313 [[10.7093  3.3803 20.8587 28.9697 11.9868 12.3476 11.7477]] Counter({4: 1, 3: 1, 1: 1})\n",
      "3 r=1.0 p=0.6843158006668091 [[25.1621  1.8596  2.5722 26.4275 28.8412  3.69   11.4474]] Counter({5: 1})\n",
      "\n",
      "train_step: 400: {'action_accuracy': Array(0.6445064, dtype=float32), 'reward_mse': Array(0.09108582, dtype=float32), 'loss': Array(1.0483239, dtype=float32)}\n",
      "0 r=1.0 p=0.6039731502532959 [[22.2117 12.1982 12.3513 13.9559 15.9282 13.0642 10.2906]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
      "1 r=1.0 p=0.5562941431999207 [[ 7.3003  7.2835 23.4943  0.0811 15.0751 22.9768 23.7889]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
      "2 r=1.0 p=0.3006491959095001 [[30.2607  0.0835 31.6072 33.8494  0.7095  0.5865  2.9033]] Counter({4: 1, 3: 1, 1: 1})\n",
      "3 r=1.0 p=0.7119238376617432 [[11.4186  0.0118  0.0098  1.8061 86.2573  0.0556  0.4408]] Counter({5: 1})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_train_step(print_logits=False, num_steps=400):\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(10))\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(1))\n",
    "    prediction_model = PredictionModel(state_embedder, action_embedder, rngs=nnx.Rngs(2))\n",
    "    optimizer = nnx.Optimizer(prediction_model, optax.adamw(learning_rate=0.0005))\n",
    "    metrics = nnx.MultiMetric(\n",
    "        action_accuracy=LogitAccuracy('action_logits', 'action_labels'),\n",
    "        reward_mse=MeanSquaredError('reward_pred', 'reward_labels'),\n",
    "        loss=nnx.metrics.Average('loss'))\n",
    "    \n",
    "    state_action_counts = {}\n",
    "    state_reward_counts = {}\n",
    "    for i in range(4):\n",
    "        state = batch_full['state'][i]\n",
    "        state_action_counts[i] = Counter(a.item() for s,a in zip(batch_full['state'], batch_full['action']) if (s == state).all())\n",
    "        state_reward_counts[i] = Counter(r.item() for s,r in zip(batch_full['state'], batch_full['reward']) if (s == state).all())\n",
    "\n",
    "    for i in range(1, num_steps+1):\n",
    "        train_step(prediction_model, optimizer, metrics, batch_full)\n",
    "        if i == 1 or i % 200 == 0 or i == num_steps:\n",
    "            print(f'train_step: {i}: {metrics.compute()}')\n",
    "            if print_logits:\n",
    "                for i in range(4):                \n",
    "                    state = batch_full['state'][i]\n",
    "                    action_probs = prediction_model.action_probs(jnp.array([state]))\n",
    "                    reward_pred = prediction_model.reward_pred(jnp.array([state])).item()\n",
    "                    reward_true = batch_full['reward'][i].item()\n",
    "                    print(i, f'r={reward_true} p={reward_pred}', jnp.array_str(action_probs * 100, precision=4, max_line_width=100, suppress_small=True), state_action_counts[i], state_reward_counts[i])\n",
    "                print()\n",
    "\n",
    "test_train_step(print_logits=True, num_steps=400)\n",
    "\n",
    "# train_step: 1: {'action_accuracy': Array(0.14363885, dtype=float32), 'reward_mse': Array(1.2103255, dtype=float32), 'loss': Array(3.1888344, dtype=float32)}\n",
    "# 0 r=1.0 p=0.009542055428028107 [[14.301  14.3019 14.2707 14.2778 14.2718 14.2823 14.2946]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 r=1.0 p=0.239242821931839 [[15.7015 14.4646 14.3248 13.5308 13.8753 13.7428 14.3602]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 r=1.0 p=0.38645169138908386 [[16.3885 15.3902 13.6702 13.2374 14.175  12.9441 14.1946]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 r=1.0 p=0.5279813408851624 [[16.5539 16.2118 13.3792 13.607  13.9671 12.0987 14.1822]] Counter({5: 1})\n",
    "\n",
    "# train_step: 200: {'action_accuracy': Array(0.39684907, dtype=float32), 'reward_mse': Array(0.3694912, dtype=float32), 'loss': Array(1.9460464, dtype=float32)}\n",
    "# 0 r=1.0 p=0.2352880835533142 [[23.4505 11.7681 12.0107 15.0266 15.1127 12.3827 10.2487]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 r=1.0 p=0.1304871290922165 [[ 8.4951  9.1253 24.1409  2.3545 11.7189 24.5514 19.6139]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 r=1.0 p=-0.51841139793396 [[12.4069  5.542  19.5634 17.891  13.2847 15.4737 15.8383]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 r=1.0 p=0.5774312019348145 [[21.8451  4.6146  3.4356 13.3274 22.2893  7.1891 27.2988]] Counter({5: 1})\n",
    "\n",
    "# train_step: 400: {'action_accuracy': Array(0.6348518, dtype=float32), 'reward_mse': Array(0.2586214, dtype=float32), 'loss': Array(1.2657048, dtype=float32)}\n",
    "# 0 r=1.0 p=0.24247753620147705 [[22.9135 12.1181 12.1923 13.9984 15.749  13.0248 10.0039]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 r=1.0 p=0.17915339767932892 [[ 7.1621  7.4489 23.7718  0.0898 14.4461 23.1061 23.9753]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 r=1.0 p=-0.3926801085472107 [[32.4896  0.9777 29.2641 33.8393  0.1045  1.6182  1.7066]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 r=1.0 p=0.8595808744430542 [[17.7365  0.0774  0.0108  3.722  75.0556  1.3197  2.0781]] Counter({5: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a34a398d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# for step, batch in enumerate(train_ds.as_numpy_iterator()):\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m train_batches:\n\u001b[1;32m     27\u001b[0m   \u001b[38;5;66;03m# with jax.disable_jit():\u001b[39;00m\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;66;03m#   train_step(state_embedder, action_embedder, optimizer, metrics, batch)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m   train_step(\u001b[43mstate_embedder\u001b[49m, action_embedder, optimizer, metrics, batch)\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (step \u001b[38;5;241m%\u001b[39m eval_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m train_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# One training epoch has passed.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Log the training metrics.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# Compute the metrics.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'state_embedder' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: How do we stream a bunch of different batches?\n",
    "# train_batches = [(i, batch) for i in range(2000)]\n",
    "# test_batches = [(i, batch) for i in range(100)]\n",
    "\n",
    "#train_batches = [(i, batch_repeated) for i in range(2000)]\n",
    "#test_batches = [(i, batch_repeated) for i in range(100)]\n",
    "\n",
    "# NUM_TRAIN_BATCHES = 2000\n",
    "NUM_TRAIN_BATCHES = 1\n",
    "train_batches = [(i, batch_full) for i in range(NUM_TRAIN_BATCHES)]\n",
    "test_batches = [(i, batch_full) for i in range(100)]\n",
    "\n",
    "\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "  'train_accuracy': [],\n",
    "  'test_loss': [],\n",
    "  'test_accuracy': [],\n",
    "}\n",
    "\n",
    "train_steps = 10000\n",
    "eval_every = 200\n",
    "batch_size = 32\n",
    "\n",
    "# for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "for step, batch in train_batches:\n",
    "  # with jax.disable_jit():\n",
    "  #   train_step(state_embedder, action_embedder, optimizer, metrics, batch)\n",
    "  train_step(state_embedder, action_embedder, optimizer, metrics, batch)\n",
    "\n",
    "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "    # Log the training metrics.\n",
    "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "    metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "    # Compute the metrics on the test set after each training epoch.\n",
    "    for _, test_batch in test_batches:\n",
    "      eval_step(state_embedder, action_embedder, metrics, test_batch)\n",
    "\n",
    "    # Log the test metrics.\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\n",
    "      f\"[train] step: {step}, \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "    )\n",
    "    # print(\n",
    "    #   f\"[test] step: {step}, \"\n",
    "    #   f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "    #   f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "    # )\n",
    "\n",
    "\n",
    "# ## Outputs with embedding normalization.\n",
    "# [train] step: 200, loss: 1.4493087530136108, accuracy: 41.806243896484375\n",
    "# [train] step: 400, loss: 0.495466947555542, accuracy: 77.71910858154297\n",
    "# [train] step: 600, loss: 0.4014337956905365, accuracy: 80.45576477050781\n",
    "# [train] step: 800, loss: 0.3858652412891388, accuracy: 80.7986831665039\n",
    "# [train] step: 1000, loss: 0.38055431842803955, accuracy: 80.97492218017578\n",
    "# [train] step: 1200, loss: 0.37459635734558105, accuracy: 81.28226470947266\n",
    "# [train] step: 1400, loss: 0.3812824487686157, accuracy: 81.351806640625\n",
    "# [train] step: 1600, loss: 0.366107702255249, accuracy: 81.67510986328125\n",
    "# [train] step: 1800, loss: 0.3646901547908783, accuracy: 81.75855255126953\n",
    "# [train] step: 2000, loss: 0.3648184537887573, accuracy: 81.75786590576172\n",
    "# [train] step: 2200, loss: 0.35771697759628296, accuracy: 82.1707763671875\n",
    "# [train] step: 2400, loss: 0.3564577102661133, accuracy: 82.21227264404297\n",
    "# [train] step: 2600, loss: 0.3553982973098755, accuracy: 82.25535583496094\n",
    "# [train] step: 2800, loss: 0.3551204204559326, accuracy: 82.25627136230469\n",
    "# [train] step: 3000, loss: 0.35472187399864197, accuracy: 82.2571792602539\n",
    "# [train] step: 3200, loss: 0.35463690757751465, accuracy: 82.25672912597656\n",
    "# [train] step: 3400, loss: 0.35420480370521545, accuracy: 82.25741577148438\n",
    "# [train] step: 3600, loss: 0.35807478427886963, accuracy: 82.2020034790039\n",
    "# [train] step: 3800, loss: 0.35097551345825195, accuracy: 82.39557647705078\n",
    "# [train] step: 4000, loss: 0.3513658940792084, accuracy: 82.39649200439453\n",
    "# [train] step: 4200, loss: 0.3515341877937317, accuracy: 82.39580535888672\n",
    "# [train] step: 4400, loss: 0.35152217745780945, accuracy: 82.39762878417969\n",
    "# [train] step: 4600, loss: 0.35154837369918823, accuracy: 82.39649200439453\n",
    "# [train] step: 4800, loss: 0.3515065014362335, accuracy: 82.40766143798828\n",
    "# [train] step: 5000, loss: 0.3419311046600342, accuracy: 83.08686828613281\n",
    "# ...\n",
    "# [train] step: 19200, loss: 0.21814590692520142, accuracy: 90.15048217773438\n",
    "# [train] step: 19400, loss: 0.2181217074394226, accuracy: 90.15048217773438\n",
    "# [train] step: 19600, loss: 0.21812711656093597, accuracy: 90.15048217773438\n",
    "# [train] step: 19800, loss: 0.21816301345825195, accuracy: 90.14934539794922\n",
    "\n",
    "\n",
    "# logits: [[-0.12943102 -0.18300387 -0.31407255 -0.26985747 -0.03317889  0.8657205  -0.06745078]\n",
    "#          [-0.3108766  -0.11808072 -0.00877056 -0.12999986 -0.18233624 -0.03913596   0.86498916]]\n",
    "# probs: [[0.11739378 0.11127014 0.09760144 0.10201374 0.12925485 0.31756592  0.1249001 ]\n",
    "#         [0.09565745 0.11599759 0.12939627 0.1146232  0.10877853 0.12552617  0.31002077]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7179d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare action counts to action probabilities.\n",
    "for i in range(10):\n",
    "    state = batch_full['state'][i]\n",
    "    counts = Counter(a.item() for s,a in zip(batch_full['state'], batch_full['action']) if (s == state).all())\n",
    "    probs = state_action_model.probs(jnp.array([state]))\n",
    "    print(i, jnp.array_str(probs * 100, precision=4, max_line_width=100, suppress_small=True), counts)\n",
    "\n",
    "## No training.\n",
    "\n",
    "# 0 [[16.0473 15.4123 13.6238 13.4007 12.6302 15.335  13.5506]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 [[29.2187 19.2536 10.1736  7.9687  5.0374 18.8691  9.4789]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 [[39.9711 18.2044  6.5298  5.4635  2.1863 20.3188  7.3262]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 [[48.2277 17.6604  3.9698  3.1657  1.1044 21.3496  4.5223]] Counter({5: 1})\n",
    "# 4 [[52.5742 19.8617  1.9912  1.4634  0.372  21.7475  1.9901]] Counter({5: 1})\n",
    "# 5 [[59.2366 18.4123  1.1171  0.8116  0.1682 19.1115  1.1429]] Counter({7: 1})\n",
    "# 6 [[62.148  21.6044  0.7341  0.471   0.061  14.4665  0.515 ]] Counter({1: 1})\n",
    "# 7 [[63.5058 21.5263  0.4813  0.303   0.0317 13.7824  0.3696]] Counter({7: 1})\n",
    "# 8 [[76.6878 13.1025  0.1152  0.0922  0.006   9.8978  0.0985]] Counter({1: 1})\n",
    "# 9 [[79.2133 11.9533  0.0701  0.0548  0.0032  8.6434  0.0619]] Counter({1: 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
