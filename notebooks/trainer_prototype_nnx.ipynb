{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1440ce-9dee-447c-93a5-65b8c7e889e3",
   "metadata": {},
   "source": [
    "# Trainer NNX scratchpad\n",
    "\n",
    "Generate training data\n",
    "```\n",
    "python rgi/main.py --game connect4 --player1 random --player2 random --num_games 100 --save_trajectories\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a19b2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "print(jax.devices())\n",
    "assert jax.devices()[0].platform == 'gpu'\n",
    "\n",
    "# print wider lines to stop arrays wrapping so soon. numpy default is 75.\n",
    "jnp.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e34972",
   "metadata": {},
   "source": [
    "# Load Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41d5ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories_glob: ../data/trajectories/connect4/*.trajectory.npy\n",
      "num_trajectories: 100\n"
     ]
    }
   ],
   "source": [
    "from rgi.core import trajectory\n",
    "\n",
    "game_name = \"connect4\"\n",
    "trajectories_glob = os.path.join(\"..\", \"data\", \"trajectories\", game_name, \"*.trajectory.npy\")\n",
    "trajectories = trajectory.load_trajectories(trajectories_glob)\n",
    "\n",
    "print(f'trajectories_glob: {trajectories_glob}')\n",
    "print(f'num_trajectories: {len(trajectories)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "322895c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 1, win:  14, loss:   9, win_pct: 60.87%\n",
      "action: 2, win:   6, loss:   6, win_pct: 50.00%\n",
      "action: 3, win:   8, loss:   4, win_pct: 66.67%\n",
      "action: 4, win:  11, loss:   3, win_pct: 78.57%\n",
      "action: 5, win:  11, loss:   5, win_pct: 68.75%\n",
      "action: 6, win:   8, loss:   5, win_pct: 61.54%\n",
      "action: 7, win:   5, loss:   5, win_pct: 50.00%\n"
     ]
    }
   ],
   "source": [
    "def print_trajectory_stats():\n",
    "    c = Counter((t.actions[0].item(), t.final_rewards[0].item()) for t in trajectories)\n",
    "    for action in range(1,7+1):\n",
    "        win_count, loss_count = c[(action, 1.0)], c[(action, -1.0)]\n",
    "        print(f'action: {action}, win: {win_count:3d}, loss: {loss_count:3d}, win_pct: {win_count / (win_count + loss_count) * 100:.2f}%')\n",
    "\n",
    "print_trajectory_stats()\n",
    "\n",
    "# num_trajectories: 100\n",
    "# action: 1, win:  14, loss:   9, win_pct: 60.87%\n",
    "# action: 2, win:   6, loss:   6, win_pct: 50.00%\n",
    "# action: 3, win:   8, loss:   4, win_pct: 66.67%\n",
    "# action: 4, win:  11, loss:   3, win_pct: 78.57%\n",
    "# action: 5, win:  11, loss:   5, win_pct: 68.75%\n",
    "# action: 6, win:   8, loss:   5, win_pct: 61.54%\n",
    "# action: 7, win:   5, loss:   5, win_pct: 50.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5872e996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trajectory_steps: 2193\n"
     ]
    }
   ],
   "source": [
    "from rgi.core.trajectory import EncodedTrajectory\n",
    "\n",
    "# define trajectory NamedTuple\n",
    "from typing import NamedTuple\n",
    "class TrajectoryStep(NamedTuple):\n",
    "    move_index: int\n",
    "    state: jax.Array\n",
    "    action: jax.Array\n",
    "    next_state: jax.Array\n",
    "    reward: jax.Array\n",
    "\n",
    "\n",
    "def unroll_trajectory(encoded_trajectories: list[EncodedTrajectory]):\n",
    "    for t in encoded_trajectories:\n",
    "        for i in range(t.length - 1):\n",
    "            yield TrajectoryStep(i, t.states[i], t.actions[i], t.states[i + 1], t.state_rewards[i])\n",
    "\n",
    "all_trajecoty_steps = list(unroll_trajectory(trajectories))\n",
    "print(f'num_trajectory_steps: {len(all_trajecoty_steps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "81c8bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch = jnp.array([t.state for t in all_trajecoty_steps])\n",
    "action_batch = jnp.array([t.action for t in all_trajecoty_steps])\n",
    "reward_batch = jnp.array([t.reward for t in all_trajecoty_steps])\n",
    "batch_full = {'state': state_batch, 'action': action_batch, 'reward': reward_batch}\n",
    "\n",
    "# batch with 1 or 2 steps for testing\n",
    "batch_1 = {'state': state_batch[:1], 'action': action_batch[:1], 'reward': reward_batch[:1]}\n",
    "batch_2 = {'state': state_batch[:2], 'action': action_batch[:2], 'reward': reward_batch[:2]}\n",
    "\n",
    "# Super easy to get 100% accuracy.\n",
    "batch_repeated = {'state': jnp.tile(batch_2['state'], (1000,1)), 'action': jnp.tile(batch_2['action'], (1000,)), 'reward': jnp.tile(batch_2['reward'], (1000,))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce65ffe",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36570865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.games import connect4\n",
    "game = connect4.Connect4Game()\n",
    "serializer = connect4.Connect4Serializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44221c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4StateEmbedder(nnx.Module):\n",
    "    embedding_dim: int = 64\n",
    "    hidden_dim: int = 256\n",
    "\n",
    "    def _state_to_array(self, encoded_state_batch: jax.Array) -> jax.Array:\n",
    "        board_array = encoded_state_batch[:, :-1].reshape([-1, 6, 7])\n",
    "        return board_array\n",
    "\n",
    "    def __init__(self, *, rngs: nnx.Rngs):\n",
    "        self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs)\n",
    "        self.linear1 = nnx.Linear(6*7*64, self.hidden_dim, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(self.hidden_dim, self.embedding_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, encoded_state_batch: jax.Array):\n",
    "        x = self._state_to_array(encoded_state_batch)\n",
    "        x = x[..., None]  # Add channel dimension\n",
    "        x = nnx.relu(self.conv1(x))\n",
    "        x = nnx.relu(self.conv2(x))\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten while preserving batch dimension\n",
    "        x = nnx.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c3b36c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def test_state_embedder():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(0))\n",
    "    print(state_embedder(trajectories[0].states[:1]))\n",
    "\n",
    "test_state_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2b11059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4ActionEmbedder(nnx.Module):\n",
    "    embedding_dim: int = 64\n",
    "    num_actions: int = 7\n",
    "\n",
    "    def __init__(self, *, rngs: nnx.Rngs):\n",
    "        self.embedding = nnx.Embed(num_embeddings=self.num_actions, features=self.embedding_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, action: jax.Array) -> jax.Array:\n",
    "        # Ensure action is 0-indexed\n",
    "        action = action - 1\n",
    "        return self.embedding(action)\n",
    "    \n",
    "    def all_action_embeddings(self):\n",
    "        # return self(jnp.arange(1,self.num_actions+1))\n",
    "        return self.embedding.embedding.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e51cf2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.66427687e-02  1.09368816e-01 -1.89152256e-01  8.17345604e-02 -7.85972252e-02 -1.82855129e-01  1.57606214e-01  1.16470948e-01 -7.91573077e-02\n",
      "   1.19286872e-01 -3.06055341e-02 -2.94262134e-02 -2.04068035e-01  2.47045040e-01 -2.14104000e-02 -1.46370962e-01  3.72717567e-02  6.29766062e-02\n",
      "  -4.52014022e-02 -2.30931133e-01  1.21301249e-01  2.88546652e-01 -3.97264175e-02 -1.19131254e-02  1.30750760e-01  1.29211560e-01  3.51449512e-02\n",
      "   1.26254661e-02  7.07970336e-02  1.36475870e-02  3.25598754e-02  5.28224790e-03 -7.92904049e-02  2.16124147e-01  1.23112321e-01 -5.16295396e-02\n",
      "  -5.90516999e-03 -7.26998299e-02  1.06619947e-01  4.65621985e-02  1.09553948e-01 -3.07245195e-01  6.44185543e-02 -5.80371059e-02  1.00223973e-01\n",
      "  -4.86722216e-02  2.83067394e-02  2.01851264e-01  3.05441283e-02 -9.47859734e-02 -8.36363509e-02 -1.35540087e-02 -5.33435494e-02  7.62056708e-02\n",
      "   8.15421045e-02  1.95111567e-03 -1.68878466e-01 -1.05658814e-01 -2.38340721e-02  6.14980869e-02  7.95577615e-02 -1.57072678e-01  1.64067205e-05\n",
      "   8.12526867e-02]]\n",
      "[[ 0.15725625 -0.16321151  0.00577787 -0.13705109  0.04763619]\n",
      " [-0.10096847  0.0897982   0.17255177  0.04128496  0.2138293 ]\n",
      " [ 0.01521776 -0.06541049  0.02257618 -0.23189619  0.18868035]\n",
      " [ 0.15271325 -0.25990918 -0.13089019  0.00996069  0.17892274]\n",
      " [ 0.07929838  0.02261104 -0.15678175  0.15027006 -0.07462826]\n",
      " [ 0.06664277  0.10936882 -0.18915226  0.08173456 -0.07859723]\n",
      " [ 0.08239102 -0.19150715 -0.06263629 -0.09114807 -0.19348153]]\n"
     ]
    }
   ],
   "source": [
    "def test_action_embedder():\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(0))\n",
    "    print(action_embedder(trajectories[0].actions[:1]))\n",
    "    print(action_embedder.all_action_embeddings()[:,:5])    \n",
    "\n",
    "test_action_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21b1d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel(nnx.Module):\n",
    "    \"\"\"Model to predict action probabilities and reward of the current state.\"\"\"\n",
    "    state_embedder: Connect4StateEmbedder\n",
    "    action_embedder: Connect4ActionEmbedder\n",
    "\n",
    "    embedding_dim: int = 64\n",
    "    num_actions: int = 7\n",
    "\n",
    "    def __init__(self, state_embedder: Connect4StateEmbedder, action_embedder: Connect4ActionEmbedder, *, rngs: nnx.Rngs):\n",
    "        self.state_embedder = state_embedder\n",
    "        self.action_embedder = action_embedder\n",
    "        self.reward_head = nnx.Linear(self.embedding_dim, 1, rngs=rngs)\n",
    "\n",
    "    def action_logits(self, state_batch: jax.Array) -> jax.Array:\n",
    "        state_embeddings = self.state_embedder(state_batch)\n",
    "        all_action_embeddings = self.action_embedder.all_action_embeddings()\n",
    "        logits = state_embeddings @ all_action_embeddings.T\n",
    "        return logits\n",
    "    \n",
    "    def action_probs(self, state_batch: jax.Array) -> jax.Array:\n",
    "        logits = self.action_logits(state_batch)\n",
    "        return jax.nn.softmax(logits)\n",
    "    \n",
    "    def reward_pred(self, state_batch: jax.Array) -> jax.Array:\n",
    "        state_embeddings = self.state_embedder(state_batch)\n",
    "        return self.reward_head(state_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7cba82e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_logits:\n",
      " [[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.05365274  0.04191583 -0.05556455 -0.0902497  -0.04111161  0.00694049  0.01304062]]\n",
      "\n",
      "action_probs:\n",
      " [[0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715]\n",
      " [0.13876377 0.15267958 0.13849871 0.13377723 0.14051497 0.14743187 0.14833395]]\n",
      "\n",
      "reward_pred:\n",
      " [[ 0.        ]\n",
      " [-0.06306502]]\n"
     ]
    }
   ],
   "source": [
    "def test_state_action_model():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(0))\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(1))\n",
    "    state_action_model = PredictionModel(state_embedder, action_embedder, rngs=nnx.Rngs(2))\n",
    "    print('action_logits:\\n', state_action_model.action_logits(state_batch[:2]))\n",
    "    print('\\naction_probs:\\n', state_action_model.action_probs(state_batch[:2]))\n",
    "    print('\\nreward_pred:\\n', state_action_model.reward_pred(state_batch[:2]))\n",
    "\n",
    "test_state_action_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7637b",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a8d5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "@nnx.jit\n",
    "def l2_loss(model, alpha=0.001):\n",
    "    loss = sum(\n",
    "        (alpha * (w ** 2).sum()) \n",
    "        for w in jax.tree.leaves(nnx.state(model, nnx.Param))\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def loss_fn(prediction_model: PredictionModel, batch, l2_weight: float = 1e-4):\n",
    "    action_logits = prediction_model.action_logits(batch['state'])\n",
    "    action_labels = batch['action']-1\n",
    "    action_data_loss = optax.softmax_cross_entropy_with_integer_labels(logits=action_logits, labels=action_labels).mean()\n",
    "\n",
    "    reward_pred = prediction_model.reward_pred(batch['state'])\n",
    "    reward_labels = batch['reward']\n",
    "    reward_data_loss = ((reward_labels - reward_pred.squeeze())**2).mean()\n",
    "\n",
    "    parameter_loss_l2 = l2_weight * l2_loss(prediction_model)\n",
    "    \n",
    "    total_loss = action_data_loss + reward_data_loss + parameter_loss_l2\n",
    "    return total_loss, action_logits\n",
    "\n",
    "@nnx.jit\n",
    "def loss_fn_2(prediction_model: PredictionModel, batch, l2_weight: float = 1e-4):\n",
    "    action_logits = prediction_model.action_logits(batch['state'])\n",
    "    action_labels = batch['action']-1\n",
    "    action_data_loss = optax.softmax_cross_entropy_with_integer_labels(logits=action_logits, labels=action_labels).mean()\n",
    "\n",
    "    reward_pred = prediction_model.reward_pred(batch['state'])\n",
    "    reward_labels = batch['reward']\n",
    "    reward_data_loss = ((reward_labels - reward_pred.squeeze())**2).mean()\n",
    "\n",
    "    parameter_loss_l2 = l2_weight * l2_loss(prediction_model)\n",
    "    \n",
    "    total_loss = action_data_loss + reward_data_loss + parameter_loss_l2\n",
    "    return total_loss, (action_logits, reward_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29ed3a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.9291308\n",
      "action_logits:  [[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.05365274  0.04191583 -0.05556455 -0.0902497  -0.04111161  0.00694049  0.01304062]]\n",
      "reward_pred:  [[ 0.        ]\n",
      " [-0.06306502]]\n"
     ]
    }
   ],
   "source": [
    "def test_loss_fn():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(0))\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(1))\n",
    "    prediction_model = PredictionModel(state_embedder, action_embedder, rngs=nnx.Rngs(2))\n",
    "    loss, (action_logits, reward_pred) = loss_fn_2(prediction_model, batch_2)\n",
    "    print('loss: ', loss)\n",
    "    print('action_logits: ', action_logits)\n",
    "    print('reward_pred: ', reward_pred)\n",
    "\n",
    "with jax.disable_jit():\n",
    "    test_loss_fn()\n",
    "\n",
    "\n",
    "# loss:  1.927142\n",
    "# logits:  [[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
    "#  [-0.05365274  0.04191583 -0.05556455 -0.0902497  -0.04111161  0.00694049  0.01304064]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae318f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(prediction_model: PredictionModel, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn_2, has_aux=True)\n",
    "  (loss, (logits, reward_pred)), grads = grad_fn(prediction_model, batch)\n",
    "  metrics.update(loss=loss,\n",
    "    logits=logits, labels=batch['action']-1,\n",
    "    reward_pred=reward_pred, reward_labels=batch['reward'])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(prediction_model: PredictionModel, metrics: nnx.MultiMetric, batch):\n",
    "  loss, (logits, reward_pred) = loss_fn_2(prediction_model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch['action'])  # In-place updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a788f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3333334\n",
      "nan\n",
      "{'action': Array(0.5, dtype=float32), 'reward_mse': Array(1.3333334, dtype=float32), 'loss': Array(1., dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "from abc import ABC, abstractmethod\n",
    "from typing_extensions import override\n",
    "\n",
    "class TwoValueAverageMetric(nnx.Metric, ABC):\n",
    "    total: nnx.metrics.MetricState\n",
    "    count: nnx.metrics.MetricState\n",
    "\n",
    "    def __init__(self, argname_1: str, argname_2: str):\n",
    "        self.argname_1 = argname_1\n",
    "        self.argname_2 = argname_2\n",
    "        self.total = nnx.metrics.MetricState(jnp.array(0, dtype=jnp.float32))\n",
    "        self.count = nnx.metrics.MetricState(jnp.array(0, dtype=jnp.int32))\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.total.value = jnp.array(0, dtype=jnp.float32)\n",
    "        self.count.value = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    def update(self, **kwargs) -> None:\n",
    "        if self.argname_1 not in kwargs:\n",
    "            raise TypeError(f\"Expected keyword argument '{self.argname_1}'\")\n",
    "        if self.argname_2 not in kwargs:\n",
    "            raise TypeError(f\"Expected keyword argument '{self.argname_2}'\")      \n",
    "        v1, v2 = kwargs[self.argname_1], kwargs[self.argname_2]\n",
    "        self.update_pair(v1, v2)\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_pair(v1: jax.Array, v2: jax.Array) -> None:\n",
    "        raise NotImplementedError(\"Must be implemented by subclass\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute(self) -> jax.Array:\n",
    "        raise NotImplementedError(\"Must be implemented by subclass\")\n",
    "\n",
    "class MeanSquaredError(TwoValueAverageMetric):\n",
    "    @override\n",
    "    def __init__(self, argname_1:str='mse_1', argname_2:str='mse_2'):\n",
    "        super().__init__(argname_1, argname_2)\n",
    "\n",
    "    @override\n",
    "    def update_pair(self, v1: jax.Array, v2: jax.Array) -> None:\n",
    "        if v1.shape != v2.shape:\n",
    "            raise ValueError(f\"Expected shapes {v1.shape} and {v2.shape} to be equal\")\n",
    "        self.total.value += ((v1 - v2)**2).sum()\n",
    "        self.count.value += v1.size\n",
    "\n",
    "    def compute(self) -> jax.Array:\n",
    "        \"\"\"Compute and return the average.\"\"\"\n",
    "        return self.total.value / self.count.value\n",
    "\n",
    "class LogitAccuracy(TwoValueAverageMetric):\n",
    "    @override\n",
    "    def __init__(self, argname_1:str='logits', argname_2:str='labels'):\n",
    "        super().__init__(argname_1, argname_2)\n",
    "\n",
    "    @override\n",
    "    def update_pair(self, logits: jax.Array, labels: jax.Array) -> None:\n",
    "        if logits.ndim != labels.ndim + 1:\n",
    "            raise ValueError(f'Expected logits.ndim==labels.ndim+1, got {logits.ndim} and {labels.ndim}')\n",
    "\n",
    "        self.total.value += (logits.argmax(axis=-1) == labels).sum()\n",
    "        self.count.value += labels.size\n",
    "\n",
    "    def compute(self) -> jax.Array:\n",
    "        \"\"\"Compute and return the average.\"\"\"\n",
    "        return self.total.value / self.count.value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_mean_squared_error():\n",
    "    mse = MeanSquaredError('reward_true', 'reward_pred')\n",
    "    mse.update(reward_true=jnp.array([1, 2, 3]), reward_pred=jnp.array([1, 2, 5]))\n",
    "    print(mse.compute())\n",
    "    mse.reset()\n",
    "    print(mse.compute())\n",
    "\n",
    "test_mean_squared_error()\n",
    "\n",
    "def test_multi_metric():\n",
    "    metrics = nnx.MultiMetric(\n",
    "        accuracy=nnx.metrics.Accuracy(),\n",
    "        action_accuracy=nnx.metrics.Accuracy(\"action_logits\",\"action_labels\"),\n",
    "        reward_mse=MeanSquaredError('reward_true', 'reward_pred'),\n",
    "        loss=nnx.metrics.Average('loss'))\n",
    "    # metrics = nnx.MultiMetric(action=nnx.metrics.Accuracy(), loss=nnx.metrics.Average('loss'))\n",
    "    metrics.update(\n",
    "        loss=1.0,\n",
    "        logits=jnp.array([[1, 2, 3], [4, 5, 6]]),\n",
    "        labels=jnp.array([1, 2]),\n",
    "        action_logits=jnp.array([[1, 2, 3], [4, 5, 6]]),\n",
    "        action_labels=jnp.array([1, 2]),\n",
    "        reward_true=jnp.array([1, 2, 3]),\n",
    "        reward_pred=jnp.array([1, 2, 5]))\n",
    "    print(metrics.compute())\n",
    "\n",
    "test_multi_metric()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6230b03a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected keyword argument 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(i, jnp\u001b[38;5;241m.\u001b[39marray_str(action_probs \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, max_line_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, suppress_small\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), state_action_counts[i])\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtest_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# train_step: 0: {'accuracy': Array(0.14363885, dtype=float32), 'loss': Array(1.9785088, dtype=float32)}\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 0 [[14.2948 14.2847 14.241  14.3802 14.2482 14.3175 14.2335]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 1 [[14.9753 14.4342 14.0819 15.6058 13.1944 14.3685 13.3399]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 2 [[31.2154  0.1368 34.4281 33.3913  0.0374  0.2569  0.5341]] Counter({4: 1, 3: 1, 1: 1})\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 3 [[ 1.8426  0.0026  0.0019  1.4788 96.6549  0.0186  0.0006]] Counter({5: 1})\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[109], line 16\u001b[0m, in \u001b[0;36mtest_train_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     state_action_counts[i] \u001b[38;5;241m=\u001b[39m Counter(a\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m s,a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m (s \u001b[38;5;241m==\u001b[39m state)\u001b[38;5;241m.\u001b[39mall())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m400\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/nnx/nnx/graph.py:1158\u001b[0m, in \u001b[0;36mUpdateContextManager.__call__.<locals>.update_context_manager_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_context_manager_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1157\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m-> 1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/nnx/nnx/transforms/compilation.py:343\u001b[0m, in \u001b[0;36mjit.<locals>.jit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fun)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;129m@graph\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_context(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    337\u001b[0m   pure_args, pure_kwargs \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mto_tree(\n\u001b[1;32m    338\u001b[0m     (args, kwargs),\n\u001b[1;32m    339\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m(in_shardings, kwarg_shardings),\n\u001b[1;32m    340\u001b[0m     split_fn\u001b[38;5;241m=\u001b[39m_jit_split_fn,\n\u001b[1;32m    341\u001b[0m     ctxtag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    342\u001b[0m   )\n\u001b[0;32m--> 343\u001b[0m   pure_args_out, pure_kwargs_out, pure_out \u001b[38;5;241m=\u001b[39m \u001b[43mjitted_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpure_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpure_kwargs\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m   _args_out, _kwargs_out, out \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mfrom_tree(\n\u001b[1;32m    347\u001b[0m     (pure_args_out, pure_kwargs_out, pure_out), ctxtag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    348\u001b[0m   )\n\u001b[1;32m    349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/nnx/nnx/transforms/compilation.py:111\u001b[0m, in \u001b[0;36mJitFn.__call__\u001b[0;34m(self, *pure_args, **pure_kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mpure_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpure_kwargs):\n\u001b[1;32m    109\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mfrom_tree((pure_args, pure_kwargs), ctxtag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m   args_out, kwargs_out \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mclear_non_graph_nodes((args, kwargs))\n\u001b[1;32m    114\u001b[0m   pure_args_out, pure_kwargs_out, pure_out \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mto_tree(\n\u001b[1;32m    115\u001b[0m     (args_out, kwargs_out, out),\n\u001b[1;32m    116\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_shardings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwarg_shardings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_shardings),\n\u001b[1;32m    117\u001b[0m     ctxtag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    118\u001b[0m     split_fn\u001b[38;5;241m=\u001b[39m_jit_split_fn,\n\u001b[1;32m    119\u001b[0m   )\n",
      "Cell \u001b[0;32mIn[106], line 6\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(prediction_model, optimizer, metrics, batch)\u001b[0m\n\u001b[1;32m      4\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m nnx\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn_2, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m (loss, (logits, reward_pred)), grads \u001b[38;5;241m=\u001b[39m grad_fn(prediction_model, batch)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mreward_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# In-place updates.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/nnx/nnx/training/metrics.py:388\u001b[0m, in \u001b[0;36mMultiMetric.update\u001b[0;34m(self, **updates)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# TODO: should we give the option of updating only some of the metrics and not all? e.g. if for some kwargs==None, don't do update\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# TODO: should we raise an error if a kwarg is passed into **updates that has no match with any underlying metric? e.g. user typo\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_names:\n\u001b[0;32m--> 388\u001b[0m   \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupdates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/nnx/nnx/training/metrics.py:201\u001b[0m, in \u001b[0;36mWelford.update\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"In-place update this ``Metric``. This method will use the value from\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m``kwargs[self.argname]`` to update the metric, where ``self.argname`` is\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03mdefined on construction.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    entry that maps to the value we want to use to update this metric.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m--> 201\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m values: tp\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, jax\u001b[38;5;241m.\u001b[39mArray] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margname]\n\u001b[1;32m    203\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m values\u001b[38;5;241m.\u001b[39msize\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected keyword argument 'values'"
     ]
    }
   ],
   "source": [
    "def test_train_step():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(10))\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(1))\n",
    "    prediction_model = PredictionModel(state_embedder, action_embedder, rngs=nnx.Rngs(2))\n",
    "    optimizer = nnx.Optimizer(prediction_model, optax.adamw(learning_rate=0.0005))\n",
    "    # metrics = nnx.MultiMetric(action_accuracy=nnx.metrics.Accuracy(), reward_mse=MeanSquaredError(), loss=nnx.metrics.Average('loss'))\n",
    "    metrics = nnx.MultiMetric(action_accuracy=nnx.metrics.Accuracy(), reward_mse=nnx.metrics.Welford(), loss=nnx.metrics.Average('loss'))\n",
    "    # metrics = nnx.MultiMetric(action_accuracy=nnx.metrics.Accuracy(), loss=nnx.metrics.Average('loss'))\n",
    "    \n",
    "    state_action_counts = {}    \n",
    "    for i in range(4):\n",
    "        state = batch_full['state'][i]\n",
    "        state_action_counts[i] = Counter(a.item() for s,a in zip(batch_full['state'], batch_full['action']) if (s == state).all())\n",
    "\n",
    "    for i in range(400+1):\n",
    "        train_step(prediction_model, optimizer, metrics, batch_full)\n",
    "        if i % 200 == 0:\n",
    "            print(f'train_step: {i}: {metrics.compute()}')\n",
    "            for i in range(4):                \n",
    "                state = batch_full['state'][i]\n",
    "                action_probs = prediction_model.action_probs(jnp.array([state]))\n",
    "                print(i, jnp.array_str(action_probs * 100, precision=4, max_line_width=100, suppress_small=True), state_action_counts[i])\n",
    "            print()\n",
    "\n",
    "test_train_step()\n",
    "\n",
    "# train_step: 0: {'accuracy': Array(0.14363885, dtype=float32), 'loss': Array(1.9785088, dtype=float32)}\n",
    "# 0 [[14.2948 14.2847 14.241  14.3802 14.2482 14.3175 14.2335]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 [[14.9753 14.4342 14.0819 15.6058 13.1944 14.3685 13.3399]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 [[15.3895 15.3645 12.9772 16.6033 13.0454 13.8517 12.7683]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 [[15.1959 15.988  12.1962 19.0648 12.5115 13.4398 11.6039]] Counter({5: 1})\n",
    "\n",
    "# train_step: 200: {'accuracy': Array(0.6671386, dtype=float32), 'loss': Array(0.9482211, dtype=float32)}\n",
    "# 0 [[22.9432 12.1785 11.9234 14.0172 15.9442 12.9932 10.0002]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 [[ 6.2996  7.7686 24.5144  0.1851 14.6386 22.1654 24.4284]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 [[33.8826  1.2373 30.2037 25.751   0.9681  1.9616  5.9957]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 [[27.6696  0.2975  0.3806 17.6307 52.2947  1.326   0.4009]] Counter({5: 1})\n",
    "\n",
    "# train_step: 400: {'accuracy': Array(0.7840295, dtype=float32), 'loss': Array(0.5941784, dtype=float32)}\n",
    "# 0 [[22.3311 12.1671 12.2211 14.1951 16.2631 12.9815  9.8409]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 [[ 7.1177  7.8012 24.0158  0.0328 15.7439 23.1178 22.1708]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 [[31.2154  0.1368 34.4281 33.3913  0.0374  0.2569  0.5341]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 [[ 1.8426  0.0026  0.0019  1.4788 96.6549  0.0186  0.0006]] Counter({5: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How do we stream a bunch of different batches?\n",
    "# train_batches = [(i, batch) for i in range(2000)]\n",
    "# test_batches = [(i, batch) for i in range(100)]\n",
    "\n",
    "#train_batches = [(i, batch_repeated) for i in range(2000)]\n",
    "#test_batches = [(i, batch_repeated) for i in range(100)]\n",
    "\n",
    "# NUM_TRAIN_BATCHES = 2000\n",
    "NUM_TRAIN_BATCHES = 1\n",
    "train_batches = [(i, batch_full) for i in range(NUM_TRAIN_BATCHES)]\n",
    "test_batches = [(i, batch_full) for i in range(100)]\n",
    "\n",
    "\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "  'train_accuracy': [],\n",
    "  'test_loss': [],\n",
    "  'test_accuracy': [],\n",
    "}\n",
    "\n",
    "train_steps = 10000\n",
    "eval_every = 200\n",
    "batch_size = 32\n",
    "\n",
    "# for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "for step, batch in train_batches:\n",
    "  # with jax.disable_jit():\n",
    "  #   train_step(state_embedder, action_embedder, optimizer, metrics, batch)\n",
    "  train_step(state_embedder, action_embedder, optimizer, metrics, batch)\n",
    "\n",
    "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "    # Log the training metrics.\n",
    "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "    metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "    # Compute the metrics on the test set after each training epoch.\n",
    "    for _, test_batch in test_batches:\n",
    "      eval_step(state_embedder, action_embedder, metrics, test_batch)\n",
    "\n",
    "    # Log the test metrics.\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\n",
    "      f\"[train] step: {step}, \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "    )\n",
    "    # print(\n",
    "    #   f\"[test] step: {step}, \"\n",
    "    #   f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "    #   f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "    # )\n",
    "\n",
    "\n",
    "# ## Outputs with embedding normalization.\n",
    "# [train] step: 200, loss: 1.4493087530136108, accuracy: 41.806243896484375\n",
    "# [train] step: 400, loss: 0.495466947555542, accuracy: 77.71910858154297\n",
    "# [train] step: 600, loss: 0.4014337956905365, accuracy: 80.45576477050781\n",
    "# [train] step: 800, loss: 0.3858652412891388, accuracy: 80.7986831665039\n",
    "# [train] step: 1000, loss: 0.38055431842803955, accuracy: 80.97492218017578\n",
    "# [train] step: 1200, loss: 0.37459635734558105, accuracy: 81.28226470947266\n",
    "# [train] step: 1400, loss: 0.3812824487686157, accuracy: 81.351806640625\n",
    "# [train] step: 1600, loss: 0.366107702255249, accuracy: 81.67510986328125\n",
    "# [train] step: 1800, loss: 0.3646901547908783, accuracy: 81.75855255126953\n",
    "# [train] step: 2000, loss: 0.3648184537887573, accuracy: 81.75786590576172\n",
    "# [train] step: 2200, loss: 0.35771697759628296, accuracy: 82.1707763671875\n",
    "# [train] step: 2400, loss: 0.3564577102661133, accuracy: 82.21227264404297\n",
    "# [train] step: 2600, loss: 0.3553982973098755, accuracy: 82.25535583496094\n",
    "# [train] step: 2800, loss: 0.3551204204559326, accuracy: 82.25627136230469\n",
    "# [train] step: 3000, loss: 0.35472187399864197, accuracy: 82.2571792602539\n",
    "# [train] step: 3200, loss: 0.35463690757751465, accuracy: 82.25672912597656\n",
    "# [train] step: 3400, loss: 0.35420480370521545, accuracy: 82.25741577148438\n",
    "# [train] step: 3600, loss: 0.35807478427886963, accuracy: 82.2020034790039\n",
    "# [train] step: 3800, loss: 0.35097551345825195, accuracy: 82.39557647705078\n",
    "# [train] step: 4000, loss: 0.3513658940792084, accuracy: 82.39649200439453\n",
    "# [train] step: 4200, loss: 0.3515341877937317, accuracy: 82.39580535888672\n",
    "# [train] step: 4400, loss: 0.35152217745780945, accuracy: 82.39762878417969\n",
    "# [train] step: 4600, loss: 0.35154837369918823, accuracy: 82.39649200439453\n",
    "# [train] step: 4800, loss: 0.3515065014362335, accuracy: 82.40766143798828\n",
    "# [train] step: 5000, loss: 0.3419311046600342, accuracy: 83.08686828613281\n",
    "# ...\n",
    "# [train] step: 19200, loss: 0.21814590692520142, accuracy: 90.15048217773438\n",
    "# [train] step: 19400, loss: 0.2181217074394226, accuracy: 90.15048217773438\n",
    "# [train] step: 19600, loss: 0.21812711656093597, accuracy: 90.15048217773438\n",
    "# [train] step: 19800, loss: 0.21816301345825195, accuracy: 90.14934539794922\n",
    "\n",
    "\n",
    "# logits: [[-0.12943102 -0.18300387 -0.31407255 -0.26985747 -0.03317889  0.8657205  -0.06745078]\n",
    "#          [-0.3108766  -0.11808072 -0.00877056 -0.12999986 -0.18233624 -0.03913596   0.86498916]]\n",
    "# probs: [[0.11739378 0.11127014 0.09760144 0.10201374 0.12925485 0.31756592  0.1249001 ]\n",
    "#         [0.09565745 0.11599759 0.12939627 0.1146232  0.10877853 0.12552617  0.31002077]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7179d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare action counts to action probabilities.\n",
    "for i in range(10):\n",
    "    state = batch_full['state'][i]\n",
    "    counts = Counter(a.item() for s,a in zip(batch_full['state'], batch_full['action']) if (s == state).all())\n",
    "    probs = state_action_model.probs(jnp.array([state]))\n",
    "    print(i, jnp.array_str(probs * 100, precision=4, max_line_width=100, suppress_small=True), counts)\n",
    "\n",
    "## No training.\n",
    "\n",
    "# 0 [[16.0473 15.4123 13.6238 13.4007 12.6302 15.335  13.5506]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 [[29.2187 19.2536 10.1736  7.9687  5.0374 18.8691  9.4789]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 [[39.9711 18.2044  6.5298  5.4635  2.1863 20.3188  7.3262]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 [[48.2277 17.6604  3.9698  3.1657  1.1044 21.3496  4.5223]] Counter({5: 1})\n",
    "# 4 [[52.5742 19.8617  1.9912  1.4634  0.372  21.7475  1.9901]] Counter({5: 1})\n",
    "# 5 [[59.2366 18.4123  1.1171  0.8116  0.1682 19.1115  1.1429]] Counter({7: 1})\n",
    "# 6 [[62.148  21.6044  0.7341  0.471   0.061  14.4665  0.515 ]] Counter({1: 1})\n",
    "# 7 [[63.5058 21.5263  0.4813  0.303   0.0317 13.7824  0.3696]] Counter({7: 1})\n",
    "# 8 [[76.6878 13.1025  0.1152  0.0922  0.006   9.8978  0.0985]] Counter({1: 1})\n",
    "# 9 [[79.2133 11.9533  0.0701  0.0548  0.0032  8.6434  0.0619]] Counter({1: 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
