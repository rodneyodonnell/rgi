{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1440ce-9dee-447c-93a5-65b8c7e889e3",
   "metadata": {},
   "source": [
    "# Trainer NNX scratchpad\n",
    "\n",
    "Generate training data\n",
    "```\n",
    "python rgi/main.py --game connect4 --player1 random --player2 random --num_games 100 --save_trajectories\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a19b2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "print(jax.devices())\n",
    "assert jax.devices()[0].platform == 'gpu'\n",
    "\n",
    "# print wider lines to stop arrays wrapping so soon. numpy default is 75.\n",
    "jnp.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e34972",
   "metadata": {},
   "source": [
    "# Load Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41d5ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories_glob: ../data/trajectories/connect4/*.trajectory.npy\n",
      "num_trajectories: 1100\n"
     ]
    }
   ],
   "source": [
    "from rgi.core import trajectory\n",
    "\n",
    "game_name = \"connect4\"\n",
    "trajectories_glob = os.path.join(\"..\", \"data\", \"trajectories\", game_name, \"*.trajectory.npy\")\n",
    "trajectories = trajectory.load_trajectories(trajectories_glob)\n",
    "\n",
    "print(f'trajectories_glob: {trajectories_glob}')\n",
    "print(f'num_trajectories: {len(trajectories)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "322895c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 1, win: 102, loss:  86, win_pct: 54.26%\n",
      "action: 2, win:  85, loss:  75, win_pct: 53.12%\n",
      "action: 3, win:  84, loss:  64, win_pct: 56.76%\n",
      "action: 4, win:  98, loss:  52, win_pct: 65.33%\n",
      "action: 5, win:  89, loss:  49, win_pct: 64.49%\n",
      "action: 6, win:  80, loss:  75, win_pct: 51.61%\n",
      "action: 7, win:  83, loss:  78, win_pct: 51.55%\n"
     ]
    }
   ],
   "source": [
    "def print_trajectory_stats():\n",
    "    c = Counter((t.actions[0].item(), t.final_rewards[0].item()) for t in trajectories)\n",
    "    for action in range(1,7+1):\n",
    "        win_count, loss_count = c[(action, 1.0)], c[(action, -1.0)]\n",
    "        print(f'action: {action}, win: {win_count:3d}, loss: {loss_count:3d}, win_pct: {win_count / (win_count + loss_count) * 100:.2f}%')\n",
    "\n",
    "print_trajectory_stats()\n",
    "\n",
    "# num_trajectories: 100\n",
    "# action: 1, win:  14, loss:   9, win_pct: 60.87%\n",
    "# action: 2, win:   6, loss:   6, win_pct: 50.00%\n",
    "# action: 3, win:   8, loss:   4, win_pct: 66.67%\n",
    "# action: 4, win:  11, loss:   3, win_pct: 78.57%\n",
    "# action: 5, win:  11, loss:   5, win_pct: 68.75%\n",
    "# action: 6, win:   8, loss:   5, win_pct: 61.54%\n",
    "# action: 7, win:   5, loss:   5, win_pct: 50.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5872e996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trajectory_steps: 22835\n"
     ]
    }
   ],
   "source": [
    "from rgi.core.trajectory import EncodedTrajectory\n",
    "\n",
    "# define trajectory NamedTuple\n",
    "from typing import NamedTuple\n",
    "class TrajectoryStep(NamedTuple):\n",
    "    move_index: int\n",
    "    state: jax.Array\n",
    "    action: jax.Array\n",
    "    next_state: jax.Array\n",
    "    reward: jax.Array\n",
    "\n",
    "\n",
    "# TODO: hack fixup to make reward (0,1) instead of (-1,1)\n",
    "def fixup_reward(x): return (x+1) / 2\n",
    "\n",
    "def unroll_trajectory(encoded_trajectories: list[EncodedTrajectory]):\n",
    "    for t in encoded_trajectories:\n",
    "        for i in range(t.length - 1):\n",
    "            yield TrajectoryStep(i, t.states[i], t.actions[i], t.states[i + 1], fixup_reward(t.final_rewards[0]))\n",
    "\n",
    "all_trajecoty_steps = list(unroll_trajectory(trajectories))\n",
    "print(f'num_trajectory_steps: {len(all_trajecoty_steps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch = jnp.array([t.state for t in all_trajecoty_steps])\n",
    "action_batch = jnp.array([t.action for t in all_trajecoty_steps])\n",
    "reward_batch = jnp.array([t.reward for t in all_trajecoty_steps])\n",
    "batch_full = {'state': state_batch, 'action': action_batch, 'reward': reward_batch}\n",
    "\n",
    "# batch with 1 or 2 steps for testing\n",
    "batch_1 = {'state': state_batch[:1], 'action': action_batch[:1], 'reward': reward_batch[:1]}\n",
    "batch_2 = {'state': state_batch[:2], 'action': action_batch[:2], 'reward': reward_batch[:2]}\n",
    "\n",
    "# Super easy to get 100% accuracy.\n",
    "batch_repeated = {'state': jnp.tile(batch_2['state'], (1000,1)), 'action': jnp.tile(batch_2['action'], (1000,)), 'reward': jnp.tile(batch_2['reward'], (1000,))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce65ffe",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36570865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.games import connect4\n",
    "game = connect4.Connect4Game()\n",
    "serializer = connect4.Connect4Serializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44221c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4StateEmbedder(nnx.Module):\n",
    "    embedding_dim: int = 64\n",
    "    hidden_dim: int = 256\n",
    "\n",
    "    def _state_to_array(self, encoded_state_batch: jax.Array) -> jax.Array:\n",
    "        board_array = encoded_state_batch[:, :-1].reshape([-1, 6, 7])\n",
    "        return board_array\n",
    "\n",
    "    def __init__(self, *, rngs: nnx.Rngs):\n",
    "        self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs)\n",
    "        self.linear1 = nnx.Linear(6*7*64, self.hidden_dim, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(self.hidden_dim, self.embedding_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, encoded_state_batch: jax.Array):\n",
    "        x = self._state_to_array(encoded_state_batch)\n",
    "        x = x[..., None]  # Add channel dimension\n",
    "        x = nnx.relu(self.conv1(x))\n",
    "        x = nnx.relu(self.conv2(x))\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten while preserving batch dimension\n",
    "        x = nnx.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b36c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_state_embedder():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(0))\n",
    "    print(state_embedder(trajectories[0].states[:2])[:,:5])\n",
    "\n",
    "test_state_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b11059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4ActionEmbedder(nnx.Module):\n",
    "    embedding_dim: int = 64\n",
    "    num_actions: int = 7\n",
    "\n",
    "    def __init__(self, *, rngs: nnx.Rngs):\n",
    "        self.embedding = nnx.Embed(num_embeddings=self.num_actions, features=self.embedding_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, action: jax.Array) -> jax.Array:\n",
    "        # Ensure action is 0-indexed\n",
    "        action = action - 1\n",
    "        return self.embedding(action)\n",
    "    \n",
    "    def all_action_embeddings(self):\n",
    "        # return self(jnp.arange(1,self.num_actions+1))\n",
    "        return self.embedding.embedding.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cf2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_action_embedder():\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(0))\n",
    "    print(action_embedder(trajectories[0].actions[:1]))\n",
    "    print(action_embedder.all_action_embeddings()[:,:5])    \n",
    "\n",
    "test_action_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel(nnx.Module):\n",
    "    \"\"\"Model to predict action probabilities and reward of the current state.\"\"\"\n",
    "    state_embedder: Connect4StateEmbedder\n",
    "    action_embedder: Connect4ActionEmbedder\n",
    "\n",
    "    embedding_dim: int = 64\n",
    "    num_actions: int = 7\n",
    "\n",
    "    def __init__(self, state_embedder: Connect4StateEmbedder, action_embedder: Connect4ActionEmbedder, *, rngs: nnx.Rngs):\n",
    "        self.state_embedder = state_embedder\n",
    "        self.action_embedder = action_embedder\n",
    "        self.reward_head = nnx.Linear(self.embedding_dim, 1, rngs=rngs)\n",
    "\n",
    "    def action_logits(self, state_batch: jax.Array) -> jax.Array:\n",
    "        state_embeddings = self.state_embedder(state_batch)\n",
    "        all_action_embeddings = self.action_embedder.all_action_embeddings()\n",
    "        logits = state_embeddings @ all_action_embeddings.T\n",
    "        return logits\n",
    "    \n",
    "    def action_probs(self, state_batch: jax.Array) -> jax.Array:\n",
    "        logits = self.action_logits(state_batch)\n",
    "        return jax.nn.softmax(logits)\n",
    "    \n",
    "    def reward_pred(self, state_batch: jax.Array) -> jax.Array:\n",
    "        state_embeddings = self.state_embedder(state_batch)\n",
    "        return self.reward_head(state_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_state_action_model():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(0))\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(1))\n",
    "    state_action_model = PredictionModel(state_embedder, action_embedder, rngs=nnx.Rngs(2))\n",
    "    print('action_logits:\\n', state_action_model.action_logits(state_batch[:2]))\n",
    "    print('\\naction_probs:\\n', state_action_model.action_probs(state_batch[:2]))\n",
    "    print('\\nreward_pred:\\n', state_action_model.reward_pred(state_batch[:2]))\n",
    "\n",
    "test_state_action_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7637b",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "@nnx.jit\n",
    "def l2_loss(model, alpha=0.001):\n",
    "    loss = sum(\n",
    "        (alpha * (w ** 2).sum()) \n",
    "        for w in jax.tree.leaves(nnx.state(model, nnx.Param))\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def loss_fn(prediction_model: PredictionModel, batch, l2_weight: float = 1e-4):\n",
    "    action_logits = prediction_model.action_logits(batch['state'])\n",
    "    action_labels = batch['action']-1\n",
    "    action_data_loss = optax.softmax_cross_entropy_with_integer_labels(logits=action_logits, labels=action_labels).mean()\n",
    "\n",
    "    reward_pred = prediction_model.reward_pred(batch['state'])\n",
    "    reward_labels = batch['reward']\n",
    "    reward_data_loss = ((reward_labels - reward_pred.squeeze())**2).mean()\n",
    "\n",
    "    parameter_loss_l2 = l2_weight * l2_loss(prediction_model)\n",
    "    \n",
    "    total_loss = action_data_loss + reward_data_loss + parameter_loss_l2\n",
    "    return total_loss, (action_logits, reward_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_fn():\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(0))\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(1))\n",
    "    prediction_model = PredictionModel(state_embedder, action_embedder, rngs=nnx.Rngs(2))\n",
    "    loss, (action_logits, reward_pred) = loss_fn(prediction_model, batch_2)\n",
    "    print('loss: ', loss)\n",
    "    print('action_logits: ', action_logits)\n",
    "    print('reward_pred: ', reward_pred)\n",
    "\n",
    "with jax.disable_jit():\n",
    "    test_loss_fn()\n",
    "\n",
    "\n",
    "# loss:  1.927142\n",
    "# logits:  [[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
    "#  [-0.05365274  0.04191583 -0.05556455 -0.0902497  -0.04111161  0.00694049  0.01304064]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae318f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(prediction_model: PredictionModel, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, (logits, reward_pred)), grads = grad_fn(prediction_model, batch)\n",
    "  metrics.update(\n",
    "    loss=loss,\n",
    "    action_logits=logits, action_labels=batch['action']-1,\n",
    "    reward_pred=reward_pred.squeeze(), reward_labels=batch['reward'])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(prediction_model: PredictionModel, metrics: nnx.MultiMetric, batch):\n",
    "  loss, (logits, reward_pred) = loss_fn(prediction_model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch['action'])  # In-place updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "from abc import ABC, abstractmethod\n",
    "from typing_extensions import override\n",
    "\n",
    "class TwoValueAverageMetric(nnx.Metric, ABC):\n",
    "    total: nnx.metrics.MetricState\n",
    "    count: nnx.metrics.MetricState\n",
    "\n",
    "    def __init__(self, argname_1: str, argname_2: str):\n",
    "        self.argname_1 = argname_1\n",
    "        self.argname_2 = argname_2\n",
    "        self.total = nnx.metrics.MetricState(jnp.array(0, dtype=jnp.float32))\n",
    "        self.count = nnx.metrics.MetricState(jnp.array(0, dtype=jnp.int32))\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.total.value = jnp.array(0, dtype=jnp.float32)\n",
    "        self.count.value = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    def update(self, **kwargs) -> None:\n",
    "        if self.argname_1 not in kwargs:\n",
    "            raise TypeError(f\"Expected keyword argument '{self.argname_1}'\")\n",
    "        if self.argname_2 not in kwargs:\n",
    "            raise TypeError(f\"Expected keyword argument '{self.argname_2}'\")      \n",
    "        v1, v2 = kwargs[self.argname_1], kwargs[self.argname_2]\n",
    "        self.update_pair(v1, v2)\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_pair(v1: jax.Array, v2: jax.Array) -> None:\n",
    "        raise NotImplementedError(\"Must be implemented by subclass\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute(self) -> jax.Array:\n",
    "        raise NotImplementedError(\"Must be implemented by subclass\")\n",
    "\n",
    "class MeanSquaredError(TwoValueAverageMetric):\n",
    "    @override\n",
    "    def __init__(self, argname_1:str='mse_1', argname_2:str='mse_2'):\n",
    "        super().__init__(argname_1, argname_2)\n",
    "\n",
    "    @override\n",
    "    def update_pair(self, v1: jax.Array, v2: jax.Array) -> None:\n",
    "        if v1.shape != v2.shape:\n",
    "            raise ValueError(f\"Expected shapes {v1.shape} and {v2.shape} to be equal\")\n",
    "        self.total.value += ((v1 - v2)**2).sum()\n",
    "        self.count.value += v1.size\n",
    "\n",
    "    def compute(self) -> jax.Array:\n",
    "        \"\"\"Compute and return the average.\"\"\"\n",
    "        return self.total.value / self.count.value\n",
    "\n",
    "class LogitAccuracy(TwoValueAverageMetric):\n",
    "    @override\n",
    "    def __init__(self, argname_1:str='logits', argname_2:str='labels'):\n",
    "        super().__init__(argname_1, argname_2)\n",
    "\n",
    "    @override\n",
    "    def update_pair(self, logits: jax.Array, labels: jax.Array) -> None:\n",
    "        if logits.ndim != labels.ndim + 1:\n",
    "            raise ValueError(f'Expected logits.ndim==labels.ndim+1, got {logits.ndim} and {labels.ndim}')\n",
    "\n",
    "        self.total.value += (logits.argmax(axis=-1) == labels).sum()\n",
    "        self.count.value += labels.size\n",
    "\n",
    "    def compute(self) -> jax.Array:\n",
    "        \"\"\"Compute and return the average.\"\"\"\n",
    "        return self.total.value / self.count.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba96d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_squared_error():\n",
    "    mse = MeanSquaredError('reward_pred', 'reward_labels')\n",
    "    mse.update(reward_pred=jnp.array([1, 2, 5]), reward_labels=jnp.array([1, 2, 3]))\n",
    "    print(mse.compute())\n",
    "    mse.reset()\n",
    "\n",
    "test_mean_squared_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multi_metric():\n",
    "    metrics = nnx.MultiMetric(\n",
    "        accuracy=nnx.metrics.Accuracy(),\n",
    "        action_accuracy=LogitAccuracy(\"action_logits\",\"action_labels\"),\n",
    "        reward_mse=MeanSquaredError('reward_pred', 'reward_labels'),\n",
    "        loss=nnx.metrics.Average('loss'))\n",
    "    # metrics = nnx.MultiMetric(action=nnx.metrics.Accuracy(), loss=nnx.metrics.Average('loss'))\n",
    "    metrics.update(\n",
    "        loss=1.0,\n",
    "        logits=jnp.array([[1, 2, 3], [4, 5, 6], [3,1,1]]),\n",
    "        labels=jnp.array([1, 2, 0]),\n",
    "        action_logits=jnp.array([[1, 2, 3], [4, 5, 6], [3,1,1]]),\n",
    "        action_labels=jnp.array([1, 2, 0]),\n",
    "        reward_pred=jnp.array([1, 2, 5]),\n",
    "        reward_labels=jnp.array([1, 2, 3]),\n",
    "        )\n",
    "    pprint(metrics.compute())\n",
    "\n",
    "test_multi_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_step(print_logits=False, num_steps=400):\n",
    "    state_embedder = Connect4StateEmbedder(rngs=nnx.Rngs(10))\n",
    "    action_embedder = Connect4ActionEmbedder(rngs=nnx.Rngs(1))\n",
    "    prediction_model = PredictionModel(state_embedder, action_embedder, rngs=nnx.Rngs(2))\n",
    "    optimizer = nnx.Optimizer(prediction_model, optax.adamw(learning_rate=0.0005))\n",
    "    metrics = nnx.MultiMetric(\n",
    "        action_accuracy=LogitAccuracy('action_logits', 'action_labels'),\n",
    "        reward_mse=MeanSquaredError('reward_pred', 'reward_labels'),\n",
    "        loss=nnx.metrics.Average('loss'))\n",
    "    \n",
    "    if print_logits:\n",
    "        state_action_counts = {}\n",
    "        state_reward_counts = {}\n",
    "        for i in range(4):\n",
    "            state = batch_full['state'][i]\n",
    "            state_action_counts[i] = Counter(a.item() for s,a in zip(batch_full['state'], batch_full['action']) if (s == state).all())\n",
    "            state_reward_counts[i] = Counter(r.item() for s,r in zip(batch_full['state'], batch_full['reward']) if (s == state).all())\n",
    "\n",
    "    for i in range(1, num_steps+1):\n",
    "        train_step(prediction_model, optimizer, metrics, batch_full)\n",
    "        if i == 1 or i % 200 == 0 or i == num_steps:\n",
    "            print(f'train_step: {i}: {metrics.compute()}')\n",
    "            if print_logits:\n",
    "                for i in range(4):                \n",
    "                    state = batch_full['state'][i]\n",
    "                    action_probs = prediction_model.action_probs(jnp.array([state]))\n",
    "                    reward_pred = prediction_model.reward_pred(jnp.array([state])).item()\n",
    "                    reward_true = batch_full['reward'][i].item()\n",
    "                    print(i, f'r={reward_true} p={reward_pred}', jnp.array_str(action_probs * 100, precision=4, max_line_width=100, suppress_small=True), state_action_counts[i], state_reward_counts[i])\n",
    "                print()\n",
    "    return prediction_model\n",
    "\n",
    "# test_train_step(print_logits=True, num_steps=2000)   # 4m31s\n",
    "# test_train_step(print_logits=False, num_steps=200)     # 23s\n",
    "prediction_model = test_train_step(print_logits=False, num_steps=1000)     # 1m52s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71641039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_model.reward_pred(jnp.array([batch_full['state'][0]]))\n",
    "\n",
    "# prediction_model.reward_pred(batch_full['state'][:1000])\n",
    "reward_zip = [(a.item(), b.item()) for (a,b) in  zip(batch_full['reward'], prediction_model.reward_pred(batch_full['state']))]\n",
    "reward_zip[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8420377",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(a for (a,b) in reward_zip))\n",
    "print(sum(a for (a,b) in reward_zip))\n",
    "print(sum(b for (a,b) in reward_zip))\n",
    "print(len(reward_zip))\n",
    "\n",
    "s_0 = game.initial_state()\n",
    "s_1 = game.next_state(s_0, 1)\n",
    "\n",
    "all_moves =[s_0] + [game.next_state(s_0, a) for a in game.all_actions()]\n",
    "all_moves_jax = [serializer.state_to_jax_array(game, s) for s in all_moves]\n",
    "# serializer.state_to_jax_array(game, s_0)\n",
    "\n",
    "prediction_model.reward_pred(jnp.array(all_moves_jax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Move 0:')\n",
    "s_0 = game.initial_state()\n",
    "j_0 = serializer.state_to_jax_array(game, s_0)\n",
    "print(j_0, prediction_model.reward_pred(jnp.array([j_0])))\n",
    "\n",
    "print('\\nMove 1:')\n",
    "for a_1 in game.all_actions():\n",
    "    s_1 = game.next_state(s_0, a_1)\n",
    "    j_1 = serializer.state_to_jax_array(game, s_1)\n",
    "    print(a_1, j_1, prediction_model.reward_pred(jnp.array([j_1])))\n",
    "\n",
    "print('\\nMove 2:')\n",
    "for a_1 in game.all_actions():\n",
    "    s_1 = game.next_state(s_0, a_1)\n",
    "    for a_2 in game.all_actions():\n",
    "        s_2 = game.next_state(s_1, a_2)\n",
    "        j_2 = serializer.state_to_jax_array(game, s_2)\n",
    "        print(a_1, a_2, j_2, prediction_model.reward_pred(jnp.array([j_2])))\n",
    "\n",
    "print('\\nMove 3:')\n",
    "for a_1 in game.all_actions():\n",
    "    s_1 = game.next_state(s_0, a_1)\n",
    "    for a_2 in game.all_actions():\n",
    "        s_2 = game.next_state(s_1, a_2)\n",
    "        for a_3 in game.all_actions():\n",
    "            s_3 = game.next_state(s_2, a_3)\n",
    "            j_3 = serializer.state_to_jax_array(game, s_3)\n",
    "            print(a_1, a_2, a_3, j_3, prediction_model.reward_pred(jnp.array([j_3])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100 trajectories.\n",
    "# train_step: 1: {'action_accuracy': Array(0.14363885, dtype=float32), 'reward_mse': Array(1.2103255, dtype=float32), 'loss': Array(3.1888344, dtype=float32)}\n",
    "# 0 r=1.0 p=0.009542055428028107 [[14.301  14.3019 14.2707 14.2778 14.2718 14.2823 14.2946]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 r=1.0 p=0.239242821931839 [[15.7015 14.4646 14.3248 13.5308 13.8753 13.7428 14.3602]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 r=1.0 p=0.38645169138908386 [[16.3885 15.3902 13.6702 13.2374 14.175  12.9441 14.1946]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 r=1.0 p=0.5279813408851624 [[16.5539 16.2118 13.3792 13.607  13.9671 12.0987 14.1822]] Counter({5: 1})\n",
    "\n",
    "# train_step: 200: {'action_accuracy': Array(0.39684907, dtype=float32), 'reward_mse': Array(0.3694912, dtype=float32), 'loss': Array(1.9460464, dtype=float32)}\n",
    "# 0 r=1.0 p=0.2352880835533142 [[23.4505 11.7681 12.0107 15.0266 15.1127 12.3827 10.2487]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 r=1.0 p=0.1304871290922165 [[ 8.4951  9.1253 24.1409  2.3545 11.7189 24.5514 19.6139]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 r=1.0 p=-0.51841139793396 [[12.4069  5.542  19.5634 17.891  13.2847 15.4737 15.8383]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 r=1.0 p=0.5774312019348145 [[21.8451  4.6146  3.4356 13.3274 22.2893  7.1891 27.2988]] Counter({5: 1})\n",
    "\n",
    "# train_step: 400: {'action_accuracy': Array(0.6348518, dtype=float32), 'reward_mse': Array(0.2586214, dtype=float32), 'loss': Array(1.2657048, dtype=float32)}\n",
    "# 0 r=1.0 p=0.24247753620147705 [[22.9135 12.1181 12.1923 13.9984 15.749  13.0248 10.0039]] Counter({1: 23, 5: 16, 4: 14, 6: 13, 3: 12, 2: 12, 7: 10})\n",
    "# 1 r=1.0 p=0.17915339767932892 [[ 7.1621  7.4489 23.7718  0.0898 14.4461 23.1061 23.9753]] Counter({7: 3, 6: 3, 3: 3, 5: 2, 1: 1, 2: 1})\n",
    "# 2 r=1.0 p=-0.3926801085472107 [[32.4896  0.9777 29.2641 33.8393  0.1045  1.6182  1.7066]] Counter({4: 1, 3: 1, 1: 1})\n",
    "# 3 r=1.0 p=0.8595808744430542 [[17.7365  0.0774  0.0108  3.722  75.0556  1.3197  2.0781]] Counter({5: 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: How do we stream a bunch of different batches?\n",
    "# # train_batches = [(i, batch) for i in range(2000)]\n",
    "# # test_batches = [(i, batch) for i in range(100)]\n",
    "\n",
    "# #train_batches = [(i, batch_repeated) for i in range(2000)]\n",
    "# #test_batches = [(i, batch_repeated) for i in range(100)]\n",
    "\n",
    "# # NUM_TRAIN_BATCHES = 2000\n",
    "# NUM_TRAIN_BATCHES = 1\n",
    "# train_batches = [(i, batch_full) for i in range(NUM_TRAIN_BATCHES)]\n",
    "# test_batches = [(i, batch_full) for i in range(100)]\n",
    "\n",
    "\n",
    "# metrics_history = {\n",
    "#   'train_loss': [],\n",
    "#   'train_accuracy': [],\n",
    "#   'test_loss': [],\n",
    "#   'test_accuracy': [],\n",
    "# }\n",
    "\n",
    "# train_steps = 10000\n",
    "# eval_every = 200\n",
    "# batch_size = 32\n",
    "\n",
    "# # for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "# for step, batch in train_batches:\n",
    "#   # with jax.disable_jit():\n",
    "#   #   train_step(state_embedder, action_embedder, optimizer, metrics, batch)\n",
    "#   train_step(state_embedder, action_embedder, optimizer, metrics, batch)\n",
    "\n",
    "#   if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "#     # Log the training metrics.\n",
    "#     for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "#       metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "#     metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "#     # Compute the metrics on the test set after each training epoch.\n",
    "#     for _, test_batch in test_batches:\n",
    "#       eval_step(state_embedder, action_embedder, metrics, test_batch)\n",
    "\n",
    "#     # Log the test metrics.\n",
    "#     for metric, value in metrics.compute().items():\n",
    "#       metrics_history[f'test_{metric}'].append(value)\n",
    "#     metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "#     print(\n",
    "#       f\"[train] step: {step}, \"\n",
    "#       f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "#       f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "#     )\n",
    "#     # print(\n",
    "#     #   f\"[test] step: {step}, \"\n",
    "#     #   f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "#     #   f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "#     # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "checkpoint_base_dir = '/workspaces/rgi/data/models/connect4'\n",
    "os.makedirs(checkpoint_base_dir, exist_ok=True)\n",
    "checkpoint_dir = os.path.join(checkpoint_base_dir, 'foo')\n",
    "ckpt_dir = ocp.test_utils.erase_and_create_empty(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76633838",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model_state = nnx.split(prediction_model)\n",
    "with ocp.StandardCheckpointer() as checkpointer:\n",
    "    checkpointer.save(ckpt_dir / 'model_state', model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093db8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "\n",
    "abstract_model = nnx.eval_shape(lambda: PredictionModel(Connect4StateEmbedder(rngs=nnx.Rngs(0)), Connect4ActionEmbedder(rngs=nnx.Rngs(1)), rngs=nnx.Rngs(2)))\n",
    "graphdef, abstract_state = nnx.split(abstract_model)\n",
    "\n",
    "with ocp.StandardCheckpointer() as checkpointer:\n",
    "    state_restored = checkpointer.restore(ckpt_dir / 'model_state', abstract_state)\n",
    "model_restored = nnx.merge(graphdef, state_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Move 0:')\n",
    "s_0 = game.initial_state()\n",
    "j_0 = serializer.state_to_jax_array(game, s_0)\n",
    "print(j_0, prediction_model.reward_pred(jnp.array([j_0])), prediction_model.action_probs(jnp.array([j_0])))\n",
    "print(j_0, model_restored.reward_pred(jnp.array([j_0])), model_restored.action_probs(jnp.array([j_0])))\n",
    "\n",
    "print('Move 3:')\n",
    "print(j_3, prediction_model.reward_pred(jnp.array([j_3])), prediction_model.action_probs(jnp.array([j_3])))\n",
    "print(j_3, model_restored.reward_pred(jnp.array([j_3])), model_restored.action_probs(jnp.array([j_3])))\n",
    "\n",
    "# Move 0:\n",
    "# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] [[0.5674922]] [[0.17131999 0.14294909 0.12640913 0.1492165  0.13335824 0.13016856 0.14657846]]\n",
    "# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] [[0.5674922]] [[0.17131999 0.14294909 0.12640913 0.1492165  0.13335824 0.13016856 0.14657846]]\n",
    "# Move 3:\n",
    "# [0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2] [[0.62254393]] [[3.2540622e-01 5.2950774e-05 3.6740914e-02 3.4891254e-01 1.9822055e-02 4.7829631e-03 2.6428244e-01]]\n",
    "# [0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2] [[0.62254393]] [[3.2540622e-01 5.2950774e-05 3.6740914e-02 3.4891254e-01 1.9822055e-02 4.7829631e-03 2.6428244e-01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_restored.__class__ == prediction_model.__class__\n",
    "model_restored == prediction_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
