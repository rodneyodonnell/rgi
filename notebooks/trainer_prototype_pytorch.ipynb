{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1440ce-9dee-447c-93a5-65b8c7e889e3",
   "metadata": {},
   "source": [
    "# Trainer pytorch scratchpad\n",
    "\n",
    "Generate training data\n",
    "```\n",
    "python rgi/main.py --game connect4 --player1 random --player2 random --num_games 100 --save_trajectories\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf8ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1355e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3b522",
   "metadata": {},
   "source": [
    "# Load Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d0fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories_glob: ../data/trajectories/connect4/*.trajectory.npy\n",
      "num_trajectories: 1100\n"
     ]
    }
   ],
   "source": [
    "# Load Trajectories (same as before)\n",
    "from rgi.core import trajectory\n",
    "\n",
    "game_name = \"connect4\"\n",
    "trajectories_glob = os.path.join(\"..\", \"data\", \"trajectories\", game_name, \"*.trajectory.npy\")\n",
    "trajectories = trajectory.load_trajectories(trajectories_glob)\n",
    "\n",
    "print(f'trajectories_glob: {trajectories_glob}')\n",
    "print(f'num_trajectories: {len(trajectories)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8071314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trajectory NamedTuple and unroll_trajectory function (same as before)\n",
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "class TrajectoryStep(NamedTuple):\n",
    "    move_index: int\n",
    "    state: torch.Tensor\n",
    "    action: torch.Tensor\n",
    "    next_state: torch.Tensor\n",
    "    reward: torch.Tensor\n",
    "\n",
    "def fixup_reward(x):\n",
    "    return (x+1) / 2\n",
    "\n",
    "@torch.jit.script\n",
    "def fixup_reward_jit(x):\n",
    "    return (x+1) / 2\n",
    "\n",
    "def unroll_trajectory_fast(encoded_trajectories: list[trajectory.EncodedTrajectory]):\n",
    "    print('xxxxxxxxxxxxxxx')\n",
    "    for t in encoded_trajectories:\n",
    "        # Convert entire arrays at once\n",
    "        states = np.array(t.states, dtype=np.float32)\n",
    "        actions = np.array(t.actions, dtype=np.int32)\n",
    "        reward_value = np.array(fixup_reward(t.final_rewards[0])) # calculate once\n",
    "        reward_value_nofn = np.array((t.final_rewards[0]+1)/2) # calculate once\n",
    "        reward_value_nofn = np.array(t.final_rewards[0]) # calculate once\n",
    "        reward_value_nofn = np.array(t.final_rewards) # calculate once\n",
    "        reward_value_nofn = np.array(t.final_rewards)[0] # calculate once\n",
    "        reward_value_nofn = (np.array(t.final_rewards)[0] +1)/2# calculate once\n",
    "        reward_value_nofn = fixup_reward(np.array(t.final_rewards[0])) # calculate once\n",
    "        reward_value_nofn = torch.tensor(np.array(t.final_rewards[0])) # calculate once\n",
    "        reward_value_nofn = fixup_reward_jit(torch.tensor(np.array(t.final_rewards[0]))) # calculate once\n",
    "        \n",
    "        for i in range(t.length - 1):\n",
    "            yield TrajectoryStep(i, \n",
    "                                 torch.from_numpy(states[i]),\n",
    "                                 # torch.from_numpy(actions[i]),\n",
    "                                 torch.tensor(actions[i], dtype=torch.int32),\n",
    "                                 torch.from_numpy(states[i + 1]),\n",
    "                                 # torch.from_numpy(reward[i])\n",
    "                                 torch.tensor(reward_value, dtype=torch.float32),\n",
    "                                 )\n",
    "\n",
    "# all_trajectory_steps_fast = list(unroll_trajectory_fast(trajectories))\n",
    "# print(f'num_trajectory_steps: {len(all_trajectory_steps_fast)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "354e27c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxxxxx\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the read only flag is not supported, should always be False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m         xx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28miter\u001b[39m)\n\u001b[1;32m      5\u001b[0m         ll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(xx)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlprun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-f unroll_trajectory_fast do_stuff(1)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# %lprun -f do_stuff -f unroll_trajectory_fast do_stuff(1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# %memit do_stuff(1)\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/rgi/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/workspaces/rgi/.venv/lib/python3.12/site-packages/line_profiler/ipython_extension.py:130\u001b[0m, in \u001b[0;36mLineProfilerMagics.lprun\u001b[0;34m(self, parameter_s)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m         \u001b[43mprofile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_ns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m         message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/rgi/.venv/lib/python3.12/site-packages/line_profiler/line_profiler.py:185\u001b[0m, in \u001b[0;36mLineProfiler.runctx\u001b[0;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_by_count()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_by_count()\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m, in \u001b[0;36mdo_stuff\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m unroll_trajectory_fast(trajectories)\n\u001b[0;32m----> 4\u001b[0m     xx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     ll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(xx)\n",
      "Cell \u001b[0;32mIn[20], line 31\u001b[0m, in \u001b[0;36munroll_trajectory_fast\u001b[0;34m(encoded_trajectories)\u001b[0m\n\u001b[1;32m     29\u001b[0m reward_value_nofn \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39marray(t\u001b[38;5;241m.\u001b[39mfinal_rewards)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;66;03m# calculate once\u001b[39;00m\n\u001b[1;32m     30\u001b[0m reward_value_nofn \u001b[38;5;241m=\u001b[39m fixup_reward(np\u001b[38;5;241m.\u001b[39marray(t\u001b[38;5;241m.\u001b[39mfinal_rewards[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;66;03m# calculate once\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m reward_value_nofn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_rewards\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# calculate once\u001b[39;00m\n\u001b[1;32m     32\u001b[0m reward_value_nofn \u001b[38;5;241m=\u001b[39m fixup_reward_jit(torch\u001b[38;5;241m.\u001b[39mtensor(t\u001b[38;5;241m.\u001b[39mfinal_rewards[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;66;03m# calculate once\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: the read only flag is not supported, should always be False"
     ]
    }
   ],
   "source": [
    "def do_stuff(n=3):\n",
    "    for _ in range(n):\n",
    "        iter = unroll_trajectory_fast(trajectories)\n",
    "        xx = list(iter)\n",
    "        ll = len(xx)\n",
    "        \n",
    "\n",
    "%lprun -f unroll_trajectory_fast do_stuff(1)\n",
    "# %lprun -f do_stuff -f unroll_trajectory_fast do_stuff(1)\n",
    "# %memit do_stuff(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d43c5fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "ERROR: Could not find file /tmp/ipykernel_661/3115989386.py\n",
      "ERROR: Could not find file /tmp/ipykernel_661/1455663398.py\n",
      "xxxxxxxxxxxxxxx\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /workspaces/rgi/.venv/lib/python3.12/site-packages/memory_profiler.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "  1185   1148.7 MiB   1148.7 MiB           2               @wraps(wrapped=func)\n",
      "  1186                                                     def wrapper(*args, **kwargs):\n",
      "  1187   1148.7 MiB      0.0 MiB           2                   prof = get_prof()\n",
      "  1188   1148.7 MiB      0.0 MiB           2                   val = prof(func)(*args, **kwargs)\n",
      "  1189   1148.7 MiB      0.0 MiB           2                   show_results_bound(prof)\n",
      "  1190   1148.7 MiB      0.0 MiB           2                   return val"
     ]
    }
   ],
   "source": [
    "# %load_ext memory_profiler\n",
    "# # %memit do_stuff(1)\n",
    "# # %mprun -f do_stuff do_stuff(1)\n",
    "\n",
    "# import memory_profiler\n",
    "# # %mprun -f do_stuff memory_profiler.profile(do_stuff)(1)\n",
    "\n",
    "# @memory_profiler.profile\n",
    "# def do_stuff(n=3):\n",
    "#     for _ in range(n):\n",
    "#         iter = unroll_trajectory_fast(trajectories)\n",
    "#         xx = list(iter)\n",
    "#         ll = len(xx)\n",
    "\n",
    "#  %mprun -f do_stuff do_stuff(1)\n",
    "# # do_stuff(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dded9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile data loading code.\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "# Your slow code here\n",
    "all_trajectory_steps_2 = list(unroll_trajectory(trajectories))\n",
    "\n",
    "profiler.disable()\n",
    "# pip install snakeviz\n",
    "# snakeviz notebooks/profile_results.prof\n",
    "# profiler.dump_stats('profile_results.prof')  # 38.6s\n",
    "profiler.dump_stats('profile_results_2.prof')\n",
    "\n",
    "stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "stats.print_stats(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83753814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b168ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batches\n",
    "state_batch = torch.stack([t.state for t in all_trajectory_steps])\n",
    "action_batch = torch.stack([t.action for t in all_trajectory_steps])\n",
    "reward_batch = torch.stack([t.reward for t in all_trajectory_steps])\n",
    "batch_full = {'state': state_batch, 'action': action_batch, 'reward': reward_batch}\n",
    "\n",
    "print(f'state shape:  {batch_full[\"state\"].shape}')\n",
    "print(f'action shape: {batch_full[\"action\"].shape}')\n",
    "print(f'reward shape: {batch_full[\"reward\"].shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07df08c",
   "metadata": {},
   "source": [
    "# Save/Load in pytorch format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "states = torch.stack([t.state for t in all_trajectory_steps])\n",
    "actions = torch.stack([t.action for t in all_trajectory_steps])\n",
    "next_states = torch.stack([t.next_state for t in all_trajectory_steps])\n",
    "rewards = torch.stack([t.reward for t in all_trajectory_steps])\n",
    "\n",
    "# Save processed data\n",
    "torch.save({\n",
    "    'states': states,\n",
    "    'actions': actions,\n",
    "    'next_states': next_states,\n",
    "    'rewards': rewards\n",
    "}, 'processed_trajectories.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = torch.load('processed_trajectories.pt', weights_only=True)\n",
    "reloaded_states = loaded_data['states']\n",
    "reloaded_actions = loaded_data['actions']\n",
    "reloaded_next_states = loaded_data['next_states']\n",
    "reloaded_rewards = loaded_data['rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, states, actions, next_states, rewards):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.next_states = next_states\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.states[idx], self.actions[idx], \n",
    "                self.next_states[idx], self.rewards[idx])\n",
    "\n",
    "# Create dataset\n",
    "dataset = TrajectoryDataset(states, actions, next_states, rewards)\n",
    "\n",
    "# Use DataLoader for efficient batching\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d96bcf",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4StateEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(64 * 6 * 7, self.hidden_dim)\n",
    "        self.linear2 = nn.Linear(self.hidden_dim, self.embedding_dim)\n",
    "        \n",
    "    def _state_to_array(self, encoded_state_batch):\n",
    "        return encoded_state_batch[:, :-1].reshape(-1, 1, 6, 7)\n",
    "    \n",
    "    def forward(self, encoded_state_batch):\n",
    "        x = self._state_to_array(encoded_state_batch)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_state_embedder():\n",
    "    state_embedder = Connect4StateEmbedder().to(device)\n",
    "    sample_states = state_batch[:2].to(device)\n",
    "    embeddings = state_embedder(sample_states)\n",
    "    print(\"Sample states shape:\", sample_states.shape)\n",
    "    print(\"State embeddings shape:\", embeddings.shape)\n",
    "    print(\"First few values of embeddings:\")\n",
    "    print(embeddings[:, :5])\n",
    "\n",
    "test_state_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4ActionEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, num_actions=7):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.embedding = nn.Embedding(num_actions, embedding_dim)\n",
    "    \n",
    "    def forward(self, action):\n",
    "        return self.embedding(action - 1)\n",
    "    \n",
    "    def all_action_embeddings(self):\n",
    "        return self.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After defining Connect4ActionEmbedder\n",
    "def test_action_embedder():\n",
    "    action_embedder = Connect4ActionEmbedder().to(device)\n",
    "    sample_actions = action_batch[:1].to(device)\n",
    "    embeddings = action_embedder(sample_actions)\n",
    "    print(\"Action embeddings shape:\", embeddings.shape)\n",
    "    print(\"Action embeddings:\")\n",
    "    print(embeddings)\n",
    "    print(\"All action embeddings:\")\n",
    "    print(action_embedder.all_action_embeddings()[:, :5])\n",
    "\n",
    "test_action_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel(nn.Module):\n",
    "    def __init__(self, state_embedder, action_embedder, embedding_dim=64, num_actions=7):\n",
    "        super().__init__()\n",
    "        self.state_embedder = state_embedder\n",
    "        self.action_embedder = action_embedder\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.reward_head = nn.Linear(self.embedding_dim, 1)\n",
    "    \n",
    "    def action_logits(self, state_batch):\n",
    "        state_embeddings = self.state_embedder(state_batch)\n",
    "        all_action_embeddings = self.action_embedder.all_action_embeddings()\n",
    "        logits = torch.matmul(state_embeddings, all_action_embeddings.t())\n",
    "        return logits\n",
    "    \n",
    "    def action_probs(self, state_batch):\n",
    "        logits = self.action_logits(state_batch)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    def reward_pred(self, state_batch):\n",
    "        state_embeddings = self.state_embedder(state_batch)\n",
    "        return self.reward_head(state_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After defining PredictionModel\n",
    "def test_prediction_model():\n",
    "    state_embedder = Connect4StateEmbedder().to(device)\n",
    "    action_embedder = Connect4ActionEmbedder().to(device)\n",
    "    prediction_model = PredictionModel(state_embedder, action_embedder).to(device)\n",
    "    \n",
    "    sample_states = state_batch[:2].to(device)\n",
    "    \n",
    "    action_logits = prediction_model.action_logits(sample_states)\n",
    "    action_probs = prediction_model.action_probs(sample_states)\n",
    "    reward_pred = prediction_model.reward_pred(sample_states)\n",
    "    \n",
    "    print(\"Action logits shape:\", action_logits.shape)\n",
    "    print(\"Action logits:\")\n",
    "    print(action_logits)\n",
    "    print(\"\\nAction probabilities:\")\n",
    "    print(action_probs)\n",
    "    print(\"\\nReward predictions:\")\n",
    "    print(reward_pred)\n",
    "\n",
    "test_prediction_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a54cb9",
   "metadata": {},
   "source": [
    "# Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bea013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_fn(prediction_model, batch, l2_weight=1e-4):\n",
    "    action_logits = prediction_model.action_logits(batch['state'])\n",
    "    action_labels = (batch['action'] - 1).long()  # Convert to Long type\n",
    "    action_data_loss = nn.functional.cross_entropy(action_logits, action_labels)\n",
    "    \n",
    "    reward_pred = prediction_model.reward_pred(batch['state']).squeeze()\n",
    "    reward_labels = batch['reward']\n",
    "    reward_data_loss = nn.functional.mse_loss(reward_pred, reward_labels)\n",
    "    \n",
    "    l2_loss = sum((p ** 2).sum() for p in prediction_model.parameters())\n",
    "    \n",
    "    total_loss = action_data_loss + reward_data_loss + l2_weight * l2_loss\n",
    "    return total_loss, (action_logits, reward_pred)\n",
    "\n",
    "# Training function\n",
    "def train_step(prediction_model, optimizer, batch):\n",
    "    prediction_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss, (logits, reward_pred) = loss_fn(prediction_model, batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), logits, reward_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babfb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "# Prepare dataset\n",
    "class TrajectoryDataset(data.Dataset):\n",
    "    def __init__(self, states, actions, rewards):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.states[idx], self.actions[idx], self.rewards[idx]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TrajectoryDataset(state_batch, action_batch, reward_batch)\n",
    "batch_size = 64  # Adjust this value based on your GPU memory\n",
    "dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Main training loop\n",
    "def train_model(print_logits=False, num_epochs=10):\n",
    "    state_embedder = Connect4StateEmbedder().to(device)\n",
    "    action_embedder = Connect4ActionEmbedder().to(device)\n",
    "    prediction_model = PredictionModel(state_embedder, action_embedder).to(device)\n",
    "    optimizer = optim.Adam(prediction_model.parameters(), lr=0.0005)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i, (states, actions, rewards) in enumerate(dataloader):\n",
    "            states, actions, rewards = states.to(device), actions.to(device), rewards.to(device)\n",
    "            batch = {'state': states, 'action': actions, 'reward': rewards}\n",
    "            \n",
    "            loss, (logits, reward_pred) = loss_fn(prediction_model, batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        if print_logits:\n",
    "            for j in range(4):\n",
    "                state = states[j].unsqueeze(0)\n",
    "                action_probs = prediction_model.action_probs(state)\n",
    "                reward_pred = prediction_model.reward_pred(state).item()\n",
    "                reward_true = rewards[j].item()\n",
    "                print(j, f'r={reward_true:.4f} p={reward_pred:.4f}', action_probs.cpu().detach().numpy())\n",
    "            print()\n",
    "    \n",
    "    return prediction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e99736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "prediction_model = train_model(print_logits=True, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(prediction_model.state_dict(), 'connect4_prediction_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843dc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = PredictionModel(Connect4StateEmbedder(), Connect4ActionEmbedder())\n",
    "loaded_model.load_state_dict(torch.load('connect4_prediction_model.pth'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b305d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loaded model\n",
    "from rgi.games import connect4\n",
    "import numpy as np\n",
    "\n",
    "game = connect4.Connect4Game()\n",
    "serializer = connect4.Connect4Serializer()\n",
    "\n",
    "print('Move 0:')\n",
    "s_0 = game.initial_state()\n",
    "state_array = serializer.state_to_jax_array(game, s_0)\n",
    "writeable_state = np.array(state_array, dtype=np.float32)  # Create a writeable copy\n",
    "j_0 = torch.tensor(writeable_state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "print(j_0, prediction_model.reward_pred(j_0).item(), prediction_model.action_probs(j_0).cpu().detach().numpy())\n",
    "\n",
    "# Load the model and move it to GPU\n",
    "loaded_model = PredictionModel(Connect4StateEmbedder(), Connect4ActionEmbedder())\n",
    "loaded_model.load_state_dict(torch.load('connect4_prediction_model.pth'))\n",
    "loaded_model = loaded_model.to(device)  # Move the loaded model to GPU\n",
    "loaded_model.eval()\n",
    "\n",
    "print(j_0, loaded_model.reward_pred(j_0).item(), loaded_model.action_probs(j_0).cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
