{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1440ce-9dee-447c-93a5-65b8c7e889e3",
   "metadata": {},
   "source": [
    "# Trainer scratchpad\n",
    "\n",
    "Generate training data\n",
    "```\n",
    "python rgi/main.py --game connect4 --player1 random --player2 random --num_games 100 --save_trajectories\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7bbecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98f1f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "\n",
    "\n",
    "assert jax.devices()[0].platform == 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80cbab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.core import game_registry\n",
    "GAMES: dict[str, game_registry.RegisteredGame[Any, Any, Any]] = game_registry.GAME_REGISTRY\n",
    "PLAYERS: dict[str, Any] = game_registry.PLAYER_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e78a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.core import trajectory\n",
    "from rgi.players.zerozero.zerozero_trainer import ZeroZeroTrainer\n",
    "from rgi.players.zerozero.zerozero_model import ZeroZeroModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d137e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.games import connect4\n",
    "from rgi import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873be208",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"connect4\"\n",
    "registered_game = GAMES[game_name]\n",
    "serializer = registered_game.serializer_fn()\n",
    "state_embedder = registered_game.state_embedder_fn()\n",
    "action_embedder = registered_game.action_embedder_fn()\n",
    "game = registered_game.game_fn()\n",
    "\n",
    "trajectories_glob = os.path.join(\"..\", \"data\", \"trajectories\", game_name, \"*.trajectory.npy\")\n",
    "trajectories = trajectory.load_trajectories(trajectories_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4398fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories_glob: ../data/trajectories/connect4/*.trajectory.npy\n",
      "num_trajectories: 100\n",
      "actions:\n",
      "  [6 7 4 5 5 7 1 7 1 1 4 5 2 4 5 1 1 2 2 4 7 7 5 7 3]\n",
      "  [4 1 6 4 4 5 1 6 2 1 6 6 1 6 7 4 5 7 2 4 6 5 1 1 2 7 4 5 7]\n",
      "  [3 4 2 5 5 2 5 6 7 4 4 1 4 5 7 7 4 2 2 5 7 4 5 3 6]\n",
      "  [7 2 7 4 6 2 4 4 6 7 1 3 2 6 6 3 2 6 7 7 1 3 2 7 1 6 2]\n",
      "  [5 4 1 5 6 3 4 3 5 3 7 6 7 7 3 7 6 7 5 6 5 2 7 3 6 6 2 5 4]\n"
     ]
    }
   ],
   "source": [
    "print(f'trajectories_glob: {trajectories_glob}')\n",
    "print(f'num_trajectories: {len(trajectories)}')\n",
    "print('actions:')\n",
    "for t in trajectories[:5]: print(f'  {t.actions}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ade58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rgi.players.zerozero import zerozero_model\n",
    "# importlib.reload(zerozero_model)\n",
    "# from rgi.players.zerozero.zerozero_model import ZeroZeroModel\n",
    "\n",
    "# from rgi.players.zerozero import zerozero_trainer\n",
    "# importlib.reload(zerozero_trainer)\n",
    "# from rgi.players.zerozero.zerozero_trainer import ZeroZeroTrainer\n",
    "\n",
    "# model = ZeroZeroModel(\n",
    "#     state_embedder=state_embedder,\n",
    "#     action_embedder=action_embedder,\n",
    "#     possible_actions=game.all_actions(),\n",
    "#     embedding_dim=64,\n",
    "#     hidden_dim=128,\n",
    "#     shared_dim=256,\n",
    "# )\n",
    "\n",
    "# trainer = ZeroZeroTrainer(model, serializer, game)\n",
    "# absolute_checkpoint_dir = os.path.abspath(os.path.join(\"data\", \"checkpoints\", game_name))\n",
    "# trainer.load_checkpoint(absolute_checkpoint_dir)\n",
    "\n",
    "# trainer.train(trajectories, num_epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Save the trained model\n",
    "# trainer.save_checkpoint(absolute_checkpoint_dir)\n",
    "# print(f\"Model training completed. Checkpoint saved to {args.checkpoint_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bcc9ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.typing import FrozenVariableDict\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "import functools\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.training import train_state, checkpoints\n",
    "import optax\n",
    "from typing import Any, Tuple, List\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import jax.tree_util as jtu\n",
    "from rgi.core.base import GameSerializer, Game\n",
    "\n",
    "from rgi.core.trajectory import load_trajectories, EncodedTrajectory\n",
    "from rgi.players.zerozero.zerozero_model import ZeroZeroModel, zerozero_loss\n",
    "from typing import Iterator\n",
    "\n",
    "TStateEmbedder = connect4.Connect4StateEmbedder\n",
    "TActionEmbedder = connect4.Connect4ActionEmbedder\n",
    "TAction = int\n",
    "TEmbedding = jax.Array\n",
    "TEncodedGameState = jax.Array\n",
    "\n",
    "class ZZActionModel(nn.Module):\n",
    "    state_embedder: connect4.Connect4StateEmbedder\n",
    "    action_embedder: connect4.Connect4ActionEmbedder\n",
    "    possible_actions: list[TAction]\n",
    "    embedding_dim: int = 64\n",
    "    hidden_dim: int = 128\n",
    "    shared_dim: int = 128\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        self.shared_state_layer: nn.Module = nn.Sequential([nn.Dense(self.shared_dim), nn.relu])\n",
    "        self.shared_action_layer: nn.Module = nn.Sequential([nn.Dense(self.shared_dim), nn.relu])\n",
    "\n",
    "        self.policy_head: nn.Module = nn.Sequential(\n",
    "            [nn.Dense(self.hidden_dim), nn.relu, nn.Dense(self.embedding_dim)])\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, state: TEncodedGameState) -> tuple[TEmbedding, jax.Array]:\n",
    "        state_embedding = self.state_embedder(state)\n",
    "        policy_embedding = self.policy_head(state_embedding)\n",
    "\n",
    "        return policy_embedding\n",
    "\n",
    "    def compute_action_logits(self, policy_embedding: TEmbedding) -> jax.Array:\n",
    "        all_action_embeddings = jnp.array([self.action_embedder(action) for action in self.possible_actions])\n",
    "        return jnp.dot(policy_embedding, jnp.transpose(all_action_embeddings))\n",
    "\n",
    "    def compute_action_probabilities(self, policy_embedding: TEmbedding) -> jax.Array:\n",
    "        return jax.nn.softmax(self.compute_action_logits(policy_embedding))\n",
    "\n",
    "\n",
    "def zz_action_loss(\n",
    "    model: ZZActionModel,\n",
    "    params: dict[str, Any],\n",
    "    state: TEncodedGameState,\n",
    "    action: TAction,\n",
    "    policy_target: jax.Array,\n",
    ") -> tuple[float, dict[str, float]]:\n",
    "\n",
    "    policy_embedding: jax.Array\n",
    "\n",
    "    policy_embedding = model.apply(params, state)  # type: ignore\n",
    "    action_probs = model.apply(\n",
    "        params,\n",
    "        method=model.compute_action_probabilities,\n",
    "        policy_embedding=policy_embedding,\n",
    "    )\n",
    "    assert isinstance(action_probs, jax.Array)\n",
    "    policy_loss = -jnp.sum(policy_target * jnp.log(action_probs + 1e-8))\n",
    "\n",
    "    total_loss = jnp.mean(policy_loss)\n",
    "    loss_dict = {\n",
    "        \"total_loss\": total_loss,\n",
    "        \"policy_loss\": jnp.mean(policy_loss),\n",
    "    }\n",
    "\n",
    "    return total_loss, loss_dict\n",
    "\n",
    "class ZZActionTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ZZActionModel,\n",
    "        serializer: connect4.Connect4Serializer,\n",
    "        game: connect4.Connect4Game,\n",
    "        learning_rate: float = 1e-4,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.serializer = serializer\n",
    "        self.game = game\n",
    "        self.optimizer = optax.adam(learning_rate)\n",
    "        self.state = None\n",
    "\n",
    "    # TODO: Rename\n",
    "    def create_train_state(self, rng: jax.random.PRNGKey) -> train_state.TrainState:\n",
    "        dummy_state = self.serializer.state_to_jax_array(self.game, self.game.initial_state())\n",
    "        dummy_action = self.serializer.action_to_jax_array(self.game, self.game.all_actions()[0])\n",
    "        # Add batch dimension to dummy inputs\n",
    "        dummy_state_batch = jnp.expand_dims(dummy_state, axis=0)\n",
    "        dummy_action_batch = jnp.expand_dims(dummy_action, axis=0)\n",
    "\n",
    "        params = self.model.init(rng, dummy_state_batch)\n",
    "        return train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=self.optimizer)\n",
    "\n",
    "\n",
    "    @jax.disable_jit()  # TODO: Remove this\n",
    "    @functools.partial(jax.jit, static_argnums=0)\n",
    "    def train_step(self, state: train_state.TrainState, batch: Tuple[Any, ...]) -> Tuple[train_state.TrainState, dict]:\n",
    "        def loss_fn(params) -> tuple[float, dict[str, float]]:\n",
    "            state_input, action, next_state, reward, policy_target = batch\n",
    "            reward = jnp.asarray(reward)  # Ensure reward is a jax.Array\n",
    "            loss, loss_dict = zz_action_loss(\n",
    "                self.model,\n",
    "                params,\n",
    "                state_input,\n",
    "                action,\n",
    "                policy_target,\n",
    "            )\n",
    "            return loss, loss_dict\n",
    "\n",
    "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "        (_, loss_dict), grads = grad_fn(state.params)\n",
    "        return state.apply_gradients(grads=grads), loss_dict\n",
    "\n",
    "    def create_batches(self, trajectories: List[EncodedTrajectory], batch_size: int) -> Iterator[Tuple[Any, ...]]:\n",
    "        states, actions, next_states, rewards, policy_targets = [], [], [], [], []\n",
    "\n",
    "        possible_actions = self.model.possible_actions\n",
    "        for trajectory in trajectories:\n",
    "            for i in range(trajectory.length - 1):\n",
    "                states.append(trajectory.states[i])\n",
    "                actions.append(trajectory.actions[i])\n",
    "                next_states.append(trajectory.states[i + 1])\n",
    "                rewards.append(trajectory.state_rewards[i])\n",
    "\n",
    "                decoded_action = self.serializer.jax_array_to_action(self.game, trajectory.actions[i])\n",
    "                decoded_action_index = possible_actions.index(decoded_action)\n",
    "                one_hot_action = jax.nn.one_hot(decoded_action_index, num_classes=len(possible_actions))\n",
    "                policy_targets.append(one_hot_action)\n",
    "\n",
    "        dataset = list(zip(states, actions, next_states, rewards, policy_targets))\n",
    "        np.random.shuffle(dataset)\n",
    "\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            yield tuple(map(np.array, zip(*dataset[i : i + batch_size])))\n",
    "\n",
    "    def train(self, trajectories: List[EncodedTrajectory], num_epochs: int, batch_size: int):\n",
    "        if self.state is None:\n",
    "            raise ValueError(\"TrainState is not initialized. Call create_train_state first.\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            batches = self.create_batches(trajectories, batch_size)\n",
    "\n",
    "            with tqdm(total=len(trajectories), desc=f\"Epoch {epoch + 1}/{num_epochs}\") as pbar:\n",
    "                for batch in batches:\n",
    "                    self.state, loss_dict = self.train_step(self.state, batch)\n",
    "                    epoch_losses.append(loss_dict[\"total_loss\"])\n",
    "                    pbar.update(batch_size)\n",
    "                    pbar.set_postfix({\"loss\": np.mean(epoch_losses)})\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} - Average Loss: {np.mean(epoch_losses):.4f}\")\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_dir: str) -> None:\n",
    "        if self.state is None:\n",
    "            raise ValueError(\"No state to save. Please train the model first.\")\n",
    "        checkpoints.save_checkpoint(checkpoint_dir, self.state, step=self.state.step, keep=3)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_dir: str) -> None:\n",
    "        \"\"\"Load checkpoint from directory. If no checkpoint is found, create a new state.\"\"\"\n",
    "        if self.state is None:\n",
    "            # We need a state of the correct type to restore from a checkpoint.\n",
    "            rng = jax.random.PRNGKey(0)\n",
    "            self.state = self.create_train_state(rng)\n",
    "        self.state = checkpoints.restore_checkpoint(checkpoint_dir, target=self.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1b5a27a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/100 [08:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ScopeParamNotFoundError",
     "evalue": "Could not find parameter named \"action_embeddings\" in scope \"/action_embedder\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamNotFoundError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m absolute_checkpoint_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, game_name))\n\u001b[1;32m     28\u001b[0m trainer\u001b[38;5;241m.\u001b[39mload_checkpoint(absolute_checkpoint_dir)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 171\u001b[0m, in \u001b[0;36mZZActionTrainer.train\u001b[0;34m(self, trajectories, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(trajectories), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m         epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    173\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(batch_size)\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 136\u001b[0m, in \u001b[0;36mZZActionTrainer.train_step\u001b[0;34m(self, state, batch)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, loss_dict\n\u001b[1;32m    135\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 136\u001b[0m (_, loss_dict), grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads), loss_dict\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[74], line 126\u001b[0m, in \u001b[0;36mZZActionTrainer.train_step.<locals>.loss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    124\u001b[0m state_input, action, next_state, reward, policy_target \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    125\u001b[0m reward \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(reward)  \u001b[38;5;66;03m# Ensure reward is a jax.Array\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m loss, loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mzz_action_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, loss_dict\n",
      "Cell \u001b[0;32mIn[74], line 68\u001b[0m, in \u001b[0;36mzz_action_loss\u001b[0;34m(model, params, state, action, policy_target)\u001b[0m\n\u001b[1;32m     65\u001b[0m policy_embedding: jax\u001b[38;5;241m.\u001b[39mArray\n\u001b[1;32m     67\u001b[0m policy_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(params, state)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_action_probabilities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action_probs, jax\u001b[38;5;241m.\u001b[39mArray)\n\u001b[1;32m     74\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mjnp\u001b[38;5;241m.\u001b[39msum(policy_target \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(action_probs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m))\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[74], line 54\u001b[0m, in \u001b[0;36mZZActionModel.compute_action_probabilities\u001b[0;34m(self, policy_embedding)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_action_probabilities\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_embedding: TEmbedding) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jax\u001b[38;5;241m.\u001b[39mArray:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_action_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_embedding\u001b[49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[74], line 50\u001b[0m, in \u001b[0;36mZZActionModel.compute_action_logits\u001b[0;34m(self, policy_embedding)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_action_logits\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_embedding: TEmbedding) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jax\u001b[38;5;241m.\u001b[39mArray:\n\u001b[0;32m---> 50\u001b[0m     all_action_embeddings \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_embedder(action) \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossible_actions])\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mdot(policy_embedding, jnp\u001b[38;5;241m.\u001b[39mtranspose(all_action_embeddings))\n",
      "Cell \u001b[0;32mIn[74], line 50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_action_logits\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_embedding: TEmbedding) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jax\u001b[38;5;241m.\u001b[39mArray:\n\u001b[0;32m---> 50\u001b[0m     all_action_embeddings \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossible_actions])\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mdot(policy_embedding, jnp\u001b[38;5;241m.\u001b[39mtranspose(all_action_embeddings))\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/app/rgi/games/connect4/connect4_embeddings.py:48\u001b[0m, in \u001b[0;36mConnect4ActionEmbedder.__call__\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: jax\u001b[38;5;241m.\u001b[39mArray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jax\u001b[38;5;241m.\u001b[39mArray:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# if not 1 <= action <= self.num_actions:\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m#    raise ValueError(f\"Action must be between 1 and {self.num_actions}\")\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     action_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstddev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action_embeddings[action \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/core/scope.py:996\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    994\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_collection_empty(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeCollectionNotFound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[0;32m--> 996\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamNotFoundError(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    997\u001b[0m value \u001b[38;5;241m=\u001b[39m init_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_rng(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m*\u001b[39minit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mput_variable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, value)\n",
      "\u001b[0;31mScopeParamNotFoundError\u001b[0m: Could not find parameter named \"action_embeddings\" in scope \"/action_embedder\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamNotFoundError)"
     ]
    }
   ],
   "source": [
    "from rgi.games import connect4\n",
    "importlib.reload(connect4)\n",
    "\n",
    "# from rgi.players.zerozero import zerozero_model\n",
    "# importlib.reload(zerozero_model)\n",
    "# from rgi.players.zerozero.zerozero_model import ZeroZeroModel\n",
    "\n",
    "# from rgi.players.zerozero import zerozero_trainer\n",
    "# importlib.reload(zerozero_trainer)\n",
    "# from rgi.players.zerozero.zerozero_trainer import ZeroZeroTrainer\n",
    "\n",
    "state_embedder = connect4.Connect4StateEmbedder()\n",
    "action_embedder = connect4.Connect4ActionEmbedder()\n",
    "serializer = connect4.Connect4Serializer()\n",
    "game = connect4.Connect4Game()\n",
    "all_actions: list[int] = game.all_actions()  # type: ignore\n",
    "\n",
    "model = ZZActionModel(\n",
    "    state_embedder=state_embedder,\n",
    "    action_embedder=action_embedder,\n",
    "    possible_actions=all_actions,\n",
    "    embedding_dim=64,\n",
    "    shared_dim=256,\n",
    ")\n",
    "\n",
    "trainer = ZZActionTrainer(model, serializer, game)\n",
    "absolute_checkpoint_dir = os.path.abspath(os.path.join(\"data\", \"checkpoints\", game_name))\n",
    "trainer.load_checkpoint(absolute_checkpoint_dir)\n",
    "\n",
    "trainer.train(trajectories, num_epochs=5, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
