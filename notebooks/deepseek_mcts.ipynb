{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ebf0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgi/players/alphazero/mcts.py\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, Generic, TypeVar, Tuple\n",
    "from rgi.core.base import Game, TGameState, TAction\n",
    "\n",
    "TGame = TypeVar(\"TGame\", bound=Game)\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "class MCTSNode(Generic[TGameState, TAction]):\n",
    "    def __init__(self, game_state: TGameState, parent: \"MCTSNode\" = None, prior: float = 0):\n",
    "        self.game_state = game_state\n",
    "        self.parent = parent\n",
    "        self.prior = prior\n",
    "        self.children: Dict[TAction, MCTSNode] = {}\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0.0\n",
    "\n",
    "    def ucb_score(self, exploration_weight: float) -> float:\n",
    "        if self.visit_count == 0:\n",
    "            return float(\"inf\")\n",
    "        return (self.value_sum / self.visit_count) + exploration_weight * self.prior * math.sqrt(self.parent.visit_count) / (1 + self.visit_count)\n",
    "\n",
    "class MCTS(Generic[TGameState, TAction]):\n",
    "    def __init__(self, game: Game[TGameState, TAction], model: \"AlphaZeroModel\", exploration_weight: float = 1.0, dirichlet_alpha: float = 0.3):\n",
    "        self.game = game\n",
    "        self.model = model\n",
    "        self.exploration_weight = exploration_weight\n",
    "        self.dirichlet_alpha = dirichlet_alpha\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        initial_state: TGameState,\n",
    "        num_simulations: int,\n",
    "        temperature: float = 1.0\n",
    "    ) -> Tuple[Dict[TAction, float], float]:\n",
    "        \"\"\"Returns action probabilities and predicted value.\"\"\"\n",
    "        root = MCTSNode(initial_state)\n",
    "\n",
    "        for _ in range(num_simulations):\n",
    "            node = root\n",
    "            search_path = [node]\n",
    "\n",
    "            # Selection\n",
    "            while node.children:\n",
    "                node = max(node.children.values(), key=lambda n: n.ucb_score(self.exploration_weight))\n",
    "                search_path.append(node)\n",
    "\n",
    "            # Expansion & Evaluation\n",
    "            if not self.game.is_terminal(node.game_state):\n",
    "                policy, value = self.model.predict(self.game.to_nn_input(node.game_state))\n",
    "                valid_actions = self.game.legal_actions(node.game_state)\n",
    "                \n",
    "                if node.parent is None:  # Root node\n",
    "                    noise = np.random.dirichlet([self.dirichlet_alpha] * len(valid_actions))\n",
    "                    policy = 0.75 * policy + 0.25 * noise\n",
    "\n",
    "                for action in valid_actions:\n",
    "                    child_state = self.game.next_state(node.game_state, action)\n",
    "                    node.children[action] = MCTSNode(\n",
    "                        child_state,\n",
    "                        parent=node,\n",
    "                        prior=policy[action]\n",
    "                    )\n",
    "            else:\n",
    "                value = self.game.reward(node.game_state, self.game.current_player_id(node.game_state))\n",
    "\n",
    "            # Backpropagation\n",
    "            self._backpropagate(search_path, value)\n",
    "\n",
    "        # Return action probabilities\n",
    "        total_visits = sum(child.visit_count for child in root.children.values())\n",
    "        action_probs = {\n",
    "            action: (child.visit_count / total_visits) ** (1 / temperature)\n",
    "            for action, child in root.children.items()\n",
    "        }\n",
    "        return action_probs, root.value_sum / root.visit_count if root.visit_count else 0\n",
    "\n",
    "    def _backpropagate(self, search_path: list[MCTSNode], value: float):\n",
    "        for node in reversed(search_path):\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            value = -value  # Alternate player perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a729383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgi/players/alphazero/model.py\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "class AlphaZeroModel(ABC):\n",
    "    @abstractmethod\n",
    "    def predict(self, game_state: np.ndarray) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Return (action probabilities, value) for the given state.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, states: list[np.ndarray], action_probs: list[np.ndarray], values: list[float]):\n",
    "        \"\"\"Update model parameters using training data.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "503ddb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgi/players/alphazero/pytorch/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PyTorchAlphaZeroNet(nn.Module):\n",
    "    def __init__(self, input_shape: tuple, num_actions: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.fc = nn.Linear(64 * input_shape[1] * input_shape[2], 256)\n",
    "        self.policy_head = nn.Linear(256, num_actions)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        policy = F.softmax(self.policy_head(x), dim=1)\n",
    "        value = torch.tanh(self.value_head(x))\n",
    "        return policy, value\n",
    "\n",
    "class PyTorchAlphaZeroModel(AlphaZeroModel):\n",
    "    def __init__(self, input_shape: tuple, num_actions: int, device: str = \"cuda\"):\n",
    "        self.net = PyTorchAlphaZeroNet(input_shape, num_actions).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters())\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, game_state: np.ndarray) -> tuple[np.ndarray, float]:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(game_state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            policy, value = self.net(state_tensor)\n",
    "            return policy.squeeze().cpu().numpy(), value.item()\n",
    "\n",
    "    def train(self, states: list[np.ndarray], action_probs: list[np.ndarray], values: list[float]):\n",
    "        states_tensor = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
    "        policies_tensor = torch.tensor(np.array(action_probs), dtype=torch.float32, device=self.device)\n",
    "        values_tensor = torch.tensor(np.array(values), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        pred_policies, pred_values = self.net(states_tensor)\n",
    "        loss = self._compute_loss(pred_policies, pred_values, policies_tensor, values_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def _compute_loss(\n",
    "        self,\n",
    "        pred_policies: torch.Tensor,\n",
    "        pred_values: torch.Tensor,\n",
    "        target_policies: torch.Tensor,\n",
    "        target_values: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        policy_loss = F.cross_entropy(pred_policies, target_policies)\n",
    "        value_loss = F.mse_loss(pred_values.squeeze(), target_values)\n",
    "        return policy_loss + value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d301382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgi/players/alphazero/pytorch/trainer.py\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "# from rgi.players.alphazero.mcts import MCTS\n",
    "import random\n",
    "\n",
    "class AlphaZeroTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        game: Game,\n",
    "        model: AlphaZeroModel,\n",
    "        num_simulations: int = 800,\n",
    "        buffer_size: int = 100000,\n",
    "        batch_size: int = 32\n",
    "    ):\n",
    "        self.game = game\n",
    "        self.model = model\n",
    "        self.mcts = MCTS(game)\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def self_play(self, num_games: int):\n",
    "        for _ in range(num_games):\n",
    "            game_state = self.game.initial_state()\n",
    "            game_history = []\n",
    "\n",
    "            while not self.game.is_terminal(game_state):\n",
    "                action_probs, _ = self.mcts.search(game_state, num_simulations=self.num_simulations)\n",
    "                action = np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
    "                game_history.append((game_state, action_probs))\n",
    "                game_state = self.game.next_state(game_state, action)\n",
    "\n",
    "            # Assign final reward to all states in the game\n",
    "            reward = self.game.reward(game_state, self.game.current_player_id(game_state))\n",
    "            for idx, (state, action_probs) in enumerate(game_history):\n",
    "                self.buffer.append((state, action_probs, reward * ((-1) ** idx)))  # Alternate perspective\n",
    "\n",
    "    def train(self, epochs: int):\n",
    "        for _ in range(epochs):\n",
    "            batch = random.sample(self.buffer, min(len(self.buffer), self.batch_size))\n",
    "            states, action_probs, values = zip(*batch)\n",
    "            self.model.train(states, action_probs, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91bdfe",
   "metadata": {},
   "source": [
    "# Run Count21 game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "755fb8d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrgi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgames\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcount21\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m count21\n\u001b[1;32m      5\u001b[0m game \u001b[38;5;241m=\u001b[39m count21\u001b[38;5;241m.\u001b[39mCount21Game()\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPyTorchAlphaZeroModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m AlphaZeroTrainer(game, model)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initial evaluation vs random\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m, in \u001b[0;36mPyTorchAlphaZeroModel.__init__\u001b[0;34m(self, input_shape, num_actions, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape: \u001b[38;5;28mtuple\u001b[39m, num_actions: \u001b[38;5;28mint\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m \u001b[43mPyTorchAlphaZeroNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m, in \u001b[0;36mPyTorchAlphaZeroNet.__init__\u001b[0;34m(self, input_shape, num_actions)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(input_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m input_shape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m256\u001b[39m, num_actions)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Run Count21 game\n",
    "\n",
    "from rgi.games.count21 import count21\n",
    "\n",
    "game = count21.Count21Game()\n",
    "model = PyTorchAlphaZeroModel(input_shape=(21,), num_actions=3)\n",
    "trainer = AlphaZeroTrainer(game, model)\n",
    "\n",
    "# Initial evaluation vs random\n",
    "evaluate_vs_random(model, game, num_games=100)\n",
    "\n",
    "# Training loop\n",
    "trainer.self_play(num_games=100)\n",
    "trainer.train(epochs=10)\n",
    "\n",
    "# Post-training evaluation\n",
    "evaluate_vs_random(model, game, num_games=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
