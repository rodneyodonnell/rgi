{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1440ce-9dee-447c-93a5-65b8c7e889e3",
   "metadata": {},
   "source": [
    "# RGI scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d01a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generic, Literal, Sequence, Type, Any\n",
    "import dataclasses\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rgi.core.base import TGameState, TAction, TPlayerState\n",
    "from rgi.core.game_registry import GAME_REGISTRY\n",
    "from rgi.core.game_registry import PLAYER_REGISTRY\n",
    "\n",
    "from rgi.core import trajectory\n",
    "from rgi.core import game_runner\n",
    "from rgi.tests import test_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bedd9",
   "metadata": {},
   "source": [
    "# Play game and record trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b363f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up game.\n",
    "from rgi.games.connect4 import connect4\n",
    "TGameState, TPlayerState, TAction = connect4.Connect4State, Literal[None], connect4.Action\n",
    "\n",
    "game = connect4.Connect4Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2670bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reloaded_trajectory_equal = True\n"
     ]
    }
   ],
   "source": [
    "# Run simple precomputed trajectory.\n",
    "importlib.reload(trajectory)\n",
    "importlib.reload(game_runner)\n",
    "\n",
    "players = [\n",
    "    test_utils.PresetPlayer[TGameState, TAction](actions=[2,2,2,2]),\n",
    "    test_utils.PresetPlayer[TGameState, TAction](actions=[1,3,4,5]),\n",
    "]\n",
    "\n",
    "runner = game_runner.GameRunner(game, players, verbose=False)\n",
    "original_trajectory = runner.run()\n",
    "\n",
    "# original_trajectory = trajectory.play_game_and_record_trajectory(game, players)\n",
    "original_trajectory.write('trajectory.npz', allow_pickle=False)\n",
    "reloaded_trajectory = trajectory.GameTrajectory.read('trajectory.npz', TGameState, TAction, allow_pickle=True)\n",
    "\n",
    "equality_checker = test_utils.EqualityChecker()\n",
    "print('reloaded_trajectory_equal =', (reloaded_trajectory_equal := equality_checker.check_equality(original_trajectory, reloaded_trajectory)))\n",
    "assert reloaded_trajectory_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09817c41",
   "metadata": {},
   "source": [
    "## Archive Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2baf6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataclasses\n",
    "from typing import Any, Type, TypeVar, Callable\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def dataclass_with_np_eq(*args: Any, **kwargs: Any) -> Callable[[Type[T]], Type[T]]:\n",
    "    \"\"\"\n",
    "    Decorator that defines a class as a dataclass with numpy-aware equality.\n",
    "    \"\"\"\n",
    "    if args and isinstance(args[0], type):\n",
    "        raise TypeError(\n",
    "            \"dataclass_with_np_eq must be called with parentheses. \"\n",
    "            \"Use @dataclass_with_np_eq() instead of @dataclass_with_np_eq\"\n",
    "        )\n",
    "\n",
    "    def wrapper(cls: Type[T]) -> Type[T]:\n",
    "        kwargs_copy = {**kwargs, 'eq': False}\n",
    "        cls = dataclasses.dataclass(**kwargs_copy)(cls)\n",
    "\n",
    "        def __eq__(self: T, other: object) -> bool:\n",
    "            if not isinstance(other, type(self)):\n",
    "                return False\n",
    "            for field in dataclasses.fields(cls):  # type: ignore\n",
    "                self_val = getattr(self, field.name)\n",
    "                other_val = getattr(other, field.name)\n",
    "                if isinstance(self_val, np.ndarray):\n",
    "                    if not np.array_equal(self_val, other_val):\n",
    "                        return False\n",
    "                elif self_val != other_val:\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        setattr(cls, '__eq__', __eq__)\n",
    "        return cls\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1c393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgi.core import archive\n",
    "importlib.reload(archive)\n",
    "import typing\n",
    "\n",
    "# mm = trajectory_archive.MemoryMappedArchive('test_archive', TGameState, TAction)\n",
    "#mm.add_trajectory(original_trajectory)\n",
    "#mm.save()\n",
    "\n",
    "original_primitive = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "primitive_item_type = int\n",
    "\n",
    "original_str = ['hello', 'world', 'foo', 'bar', 'baz', 'qux', 'quux', 'corge', 'grault', 'garply']\n",
    "str_item_type = str\n",
    "\n",
    "original_list = [[10, 20, 30], [40, 50, 60], [70, 80, 90], [100, 110, 120], [130, 140, 150], [160, 170, 180], [190, 200, 210], [220, 230, 240], [250, 260, 270], [280, 290, 300]]\n",
    "list_item_type = list[int]\n",
    "\n",
    "original_list_jagged = [[1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24, 25]]\n",
    "list_jagged_item_type = list[int]\n",
    "\n",
    "original_nested_list = [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24], [25, 26, 27]], [[28, 29, 30], [31, 32, 33], [34, 35, 36]], [[37, 38, 39], [40, 41, 42], [43, 44, 45]]]\n",
    "nested_list_item_type = list[list[int]]\n",
    "\n",
    "original_tuple = [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e'), (6, 'f'), (7, 'g'), (8, 'h'), (9, 'i'), (10, 'j')]\n",
    "tuple_item_type = tuple[int, str]\n",
    "\n",
    "original_array = [np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27], [28, 29, 30]])]\n",
    "array_item_type = np.ndarray[np.int64, np.dtype[np.int64]]\n",
    "\n",
    "\n",
    "@dataclass_with_np_eq()\n",
    "class MultiFieldItem:\n",
    "    a: int\n",
    "    b: str\n",
    "    f: float\n",
    "    t: tuple[int, str]\n",
    "    x: np.ndarray\n",
    "    y: list[int]\n",
    "\n",
    "original_multifield = [\n",
    "    MultiFieldItem(a=1, b='hello', f=1.0, t=(1, 'a'), x=np.array([1, 2, 3]), y=[1, 2, 3]),\n",
    "    MultiFieldItem(a=2, b='world', f=2.0, t=(2, 'b'), x=np.array([4, 5, 6]), y=[4, 5, 6]),\n",
    "    MultiFieldItem(a=3, b='foo', f=3.0, t=(3, 'c'), x=np.array([7, 8, 9]), y=[7, 8, 9]),\n",
    "    MultiFieldItem(a=4, b='bar', f=4.0, t=(4, 'd'), x=np.array([10, 11, 12]), y=[10, 11, 12]),\n",
    "    MultiFieldItem(a=5, b='baz', f=5.0, t=(5, 'e'), x=np.array([13, 14, 15]), y=[13, 14, 15]),\n",
    "    MultiFieldItem(a=6, b='qux', f=6.0, t=(6, 'f'), x=np.array([16, 17, 18]), y=[16, 17, 18]),\n",
    "    MultiFieldItem(a=7, b='quux', f=7.0, t=(7, 'g'), x=np.array([19, 20, 21]), y=[19, 20, 21]),\n",
    "    MultiFieldItem(a=8, b='corge', f=8.0, t=(8, 'h'), x=np.array([22, 23, 24]), y=[22, 23, 24]),\n",
    "    MultiFieldItem(a=9, b='grault', f=9.0, t=(9, 'i'), x=np.array([25, 26, 27]), y=[25, 26, 27]),\n",
    "    MultiFieldItem(a=10, b='garply', f=10.0, t=(10, 'j'), x=np.array([28, 29, 30]), y=[28, 29, 30])\n",
    "]\n",
    "multifield_item_type = MultiFieldItem\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class NestedItem:\n",
    "    t1: MultiFieldItem\n",
    "    t2: MultiFieldItem\n",
    "    list_3d: list[list[int]]\n",
    "\n",
    "original_nested = [\n",
    "    NestedItem(t1=original_multifield[0], t2=original_multifield[1], list_3d=[[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
    "    NestedItem(t1=original_multifield[2], t2=original_multifield[3], list_3d=[[10, 11, 12], [13, 14, 15], [16, 17, 18]]),\n",
    "    NestedItem(t1=original_multifield[4], t2=original_multifield[5], list_3d=[[19, 20, 21], [22, 23, 24], [25, 26, 27]]),\n",
    "    NestedItem(t1=original_multifield[6], t2=original_multifield[7], list_3d=[[28, 29, 30], [31, 32, 33], [34, 35, 36]]),\n",
    "    NestedItem(t1=original_multifield[8], t2=original_multifield[9], list_3d=[[37, 38, 39], [40, 41, 42], [43, 44, 45]]),\n",
    "]\n",
    "nested_item_type = NestedItem\n",
    "\n",
    "\n",
    "test_params = [\n",
    "    # (original_primitive, primitive_item_type),\n",
    "    # (original_str, str_item_type),\n",
    "    # (original_list, list_item_type),\n",
    "    # (original_list_jagged, list_jagged_item_type),\n",
    "    # (original_nested_list, nested_list_item_type),\n",
    "    # (original_tuple, tuple_item_type),\n",
    "    # (original_array, array_item_type),\n",
    "    (original_multifield, multifield_item_type),\n",
    "    # (original_nested, nested_item_type),\n",
    "]\n",
    "\n",
    "for original, item_type in test_params:\n",
    "    path = Path(f'{item_type.__name__}.npz')\n",
    "    serializer = archive.ArchiveSerializer(item_type)\n",
    "    serializer.save(original, path)\n",
    "\n",
    "    deserialized_archive = serializer.load_sequence(path)\n",
    "    deserialized_slice = serializer.load_sequence(path, slice(1, 3))\n",
    "\n",
    "    if item_type == np.ndarray or typing.get_origin(item_type) == np.ndarray:\n",
    "        np.testing.assert_array_equal(original, deserialized_archive)    # type: ignore\n",
    "        np.testing.assert_array_equal(original[1:3], deserialized_slice)  # type: ignore\n",
    "    else:\n",
    "        assert original == deserialized_archive, f'{item_type.__name__} failed'\n",
    "        assert original[1:3] == deserialized_slice, f'{item_type.__name__} failed'\n",
    "        assert original != original[1:3]\n",
    "\n",
    "\n",
    "    # mmap = serializer.load_mmap(path)\n",
    "    # assert original[0] == mmap[0]\n",
    "\n",
    "    # mmap = serializer.load_mmap(path)\n",
    "    # assert original[1:3] == mmap[1:3]\n",
    "\n",
    "\n",
    "# archive.MMapColumnArchive.save(list_based_archive, Path('test_archive.npz'))\n",
    "\n",
    "# mmapped_archive = archive.MMapColumnArchive(Path('test_archive.npz'), TestItem)\n",
    "# print(mmapped_archive[0])\n",
    "# print(mmapped_archive[1])\n",
    "# print(len(mmapped_archive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56149450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(slice(10))\n",
    "# print(slice(10, 20))\n",
    "# print(slice(10, 20, 3))\n",
    "\n",
    "# ss = slice(10, 20)\n",
    "# print(ss.indices(5))\n",
    "mmap = serializer.load_mmap(Path('test_archive.npz'))\n",
    "print(mmap[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf1b5384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: arrA\n",
      "  Type: <class 'numpy.memmap'>\n",
      "  Dtype, shape: float64 (10,)\n",
      "  Sample data: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "Name: arrB\n",
      "  Type: <class 'numpy.memmap'>\n",
      "  Dtype, shape: float32 (3, 4)\n",
      "  Sample data: [[-0.7570919   0.53201723  1.3183448   0.04894583]\n",
      " [ 1.2819728  -0.36975688 -0.05891573  1.1025674 ]\n",
      " [-0.5578464  -0.84648496 -1.0492903   1.756556  ]]\n"
     ]
    }
   ],
   "source": [
    "## Custom column based mmap file type\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "import json\n",
    "\n",
    "MAGIC = b\"RGF\"         # 3-byte magic string\n",
    "VERSION = b\"\\x01\"      # 1-byte version\n",
    "\n",
    "def write_data_then_metadata(filename, arrays_dict):\n",
    "    \"\"\"\n",
    "    Store multiple arrays in 'filename' with the layout:\n",
    "      - MAGIC + VERSION (4 bytes)\n",
    "      - raw array data (back-to-back)\n",
    "      - metadata block (JSON) at the end\n",
    "      - 8-byte integer: size of metadata block\n",
    "\n",
    "    arrays_dict: dict { name_str: np.ndarray }\n",
    "                 We assume you know the array shapes, dtypes, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open file for writing (truncate if exists)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        # 1) Write a magic header\n",
    "        f.write(MAGIC + VERSION)\n",
    "        # Keep track of array metadata\n",
    "        array_infos = []\n",
    "\n",
    "        # 2) Write each array's raw data, record offsets\n",
    "        for name, arr in arrays_dict.items():\n",
    "            # Align on current offset\n",
    "            data_offset = f.tell()\n",
    "            # Write array data\n",
    "            f.write(arr.tobytes(order=\"C\"))  # Write in C-contiguous order\n",
    "\n",
    "            # Store info about this array (for the metadata later)\n",
    "            array_info = {\n",
    "                \"name\": name,\n",
    "                \"dtype\": str(arr.dtype),   # e.g. 'float64'\n",
    "                \"shape\": arr.shape,       # e.g. (10,)\n",
    "                \"offset\": data_offset,    # byte offset\n",
    "            }\n",
    "            array_infos.append(array_info)\n",
    "\n",
    "        # 3) Now write the metadata as JSON\n",
    "        metadata_dict = {\"arrays\": array_infos}\n",
    "        metadata_bytes = json.dumps(metadata_dict).encode(\"utf-8\")\n",
    "\n",
    "        # Write the metadata\n",
    "        metadata_offset = f.tell()\n",
    "        f.write(metadata_bytes)\n",
    "\n",
    "        # 4) Finally, write an 8-byte integer = length of metadata block\n",
    "        metadata_size = len(metadata_bytes)\n",
    "        f.write(struct.pack(\"<Q\", metadata_size))  # little-endian 8 bytes\n",
    "        # done!\n",
    "\n",
    "\n",
    "\n",
    "def load_data_memmap(filename):\n",
    "    \"\"\"\n",
    "    Reads the single-file \"RGF\" format created by write_data_then_metadata().\n",
    "    Returns a dict: { name_str: np.memmap }.\n",
    "    \"\"\"\n",
    "    arrays_dict = {}\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        # 1) Read the magic + version\n",
    "        header = f.read(4)  # first 4 bytes\n",
    "        if len(header) < 4:\n",
    "            raise ValueError(\"File too short, not a valid RGF file.\")\n",
    "        magic_part, version_part = header[:3], header[3:]\n",
    "        if magic_part != MAGIC:\n",
    "            raise ValueError(\"File does not start with RGF magic bytes.\")\n",
    "        if version_part != VERSION:\n",
    "            raise ValueError(f\"Unsupported version: {version_part}\")\n",
    "\n",
    "        # 2) Find the metadata length by reading the last 8 bytes\n",
    "        file_size = os.fstat(f.fileno()).st_size\n",
    "        if file_size < 12:\n",
    "            raise ValueError(\"File too small to contain header + metadata length.\")\n",
    "\n",
    "        f.seek(file_size - 8)\n",
    "        metadata_size_bytes = f.read(8)\n",
    "        metadata_size = struct.unpack(\"<Q\", metadata_size_bytes)[0]\n",
    "\n",
    "        # 3) Now read the metadata block\n",
    "        metadata_start = file_size - 8 - metadata_size\n",
    "        if metadata_start < 4:\n",
    "            raise ValueError(\"Metadata overlaps with file header or data region.\")\n",
    "\n",
    "        f.seek(metadata_start)\n",
    "        metadata_bytes = f.read(metadata_size)\n",
    "        metadata_str = metadata_bytes.decode(\"utf-8\")\n",
    "        metadata_dict = json.loads(metadata_str)\n",
    "\n",
    "        # 4) For each array in metadata, create a memmap\n",
    "        for info in metadata_dict[\"arrays\"]:\n",
    "            name = info[\"name\"]\n",
    "            dtype_str = info[\"dtype\"]\n",
    "            shape = tuple(info[\"shape\"])\n",
    "            offset = info[\"offset\"]\n",
    "\n",
    "            dtype = np.dtype(dtype_str)\n",
    "            mm = np.memmap(\n",
    "                filename,\n",
    "                mode=\"r\",      # read-only\n",
    "                offset=offset,\n",
    "                shape=shape,\n",
    "                dtype=dtype,\n",
    "                order=\"C\"\n",
    "            )\n",
    "            arrays_dict[name] = mm\n",
    "\n",
    "    return arrays_dict\n",
    "\n",
    "\n",
    "\n",
    "arrays = {\n",
    "        \"arrA\": np.arange(10, dtype=np.float64),\n",
    "        \"arrB\": np.random.randn(3, 4).astype(np.float32),\n",
    "}\n",
    "write_data_then_metadata(\"mydata.rgf\", arrays)\n",
    "\n",
    "loaded_arrays = load_data_memmap(\"mydata.rgf\")\n",
    "for key, mmap_arr in loaded_arrays.items():\n",
    "    print(f\"Name: {key}\")\n",
    "    print(\"  Type:\", type(mmap_arr))\n",
    "    print(\"  Dtype, shape:\", mmap_arr.dtype, mmap_arr.shape)\n",
    "    print(\"  Sample data:\", mmap_arr[...])  # or slice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d70f66",
   "metadata": {},
   "source": [
    "# Memory map pickled row format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fff37ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 4\n",
      "Item 0: {'name': 'Alice', 'info': [1, 2, 3]}\n",
      "Item 1: Hello, world!\n",
      "Item 2: (42, 3.14, {'foo': 'bar'})\n",
      "Item 3: ['some', 'list', 'of', 'strings', 999]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def write_pickle_archive(data_filename, index_filename, objects):\n",
    "    \"\"\"\n",
    "    Write 'objects' to data_filename as raw pickled bytes (one after another).\n",
    "    Then write offsets into a .npy array (index_filename), where:\n",
    "      offsets[i] = byte offset in data_filename for the i-th object\n",
    "      offsets[-1] = final size of the data file\n",
    "\n",
    "    We do NOT store the \"count\" explicitly; we can infer from offsets array size.\n",
    "    \"\"\"\n",
    "    offsets = [0]\n",
    "\n",
    "    # 1) Write data\n",
    "    with open(data_filename, \"wb\") as fdata:\n",
    "        for obj in objects:\n",
    "            payload = pickle.dumps(obj, protocol=4)  # or higher protocol if you prefer\n",
    "            fdata.write(payload)\n",
    "            offsets.append(fdata.tell())\n",
    "\n",
    "    # 2) Convert offsets to a NumPy array and save\n",
    "    #    We'll use uint64 for large file support\n",
    "    offsets_array = np.array(offsets, dtype=np.uint64)\n",
    "    np.save(index_filename, offsets_array)  # creates index_filename (e.g. 'myindex.npy')\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def open_pickle_archive(data_filename, index_filename):\n",
    "    \"\"\"\n",
    "    Returns (get_item, count):\n",
    "      - get_item(i): retrieves i-th object from the memory-mapped data file\n",
    "      - count: number of objects\n",
    "    \"\"\"\n",
    "    # 1) Load the offsets array\n",
    "    offsets = np.load(index_filename)  # shape = (count+1,)\n",
    "    count = len(offsets) - 1\n",
    "\n",
    "    # 2) Memory-map the data file\n",
    "    mm = np.memmap(data_filename, mode=\"r\")\n",
    "\n",
    "    def get_item(i):\n",
    "        \"\"\"\n",
    "        Retrieve the i-th object by slicing [offsets[i]:offsets[i+1]] from the memmap\n",
    "        then unpickling it.\n",
    "        \"\"\"\n",
    "        if i < 0 or i >= count:\n",
    "            raise IndexError(\"Index out of range.\")\n",
    "        start = offsets[i]\n",
    "        end = offsets[i+1]\n",
    "        # This is a 'memmap' slice, which is a small ndarray of bytes\n",
    "        chunk = mm[start:end]\n",
    "        # Convert to a normal Python bytes object so pickle can read it\n",
    "        return pickle.loads(chunk.tobytes())\n",
    "\n",
    "    return get_item, count\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Some random objects\n",
    "    data = [\n",
    "        {\"name\": \"Alice\", \"info\": [1,2,3]},\n",
    "        \"Hello, world!\",\n",
    "        (42, 3.14, {\"foo\": \"bar\"}),\n",
    "        [\"some\", \"list\", \"of\", \"strings\", 999],\n",
    "    ]\n",
    "\n",
    "    # Write them out\n",
    "    write_pickle_archive(\"mydata.bin\", \"mydata_idx.npy\", data)\n",
    "\n",
    "    # Read them back\n",
    "    get_item, total = open_pickle_archive(\"mydata.bin\", \"mydata_idx.npy\")\n",
    "    print(\"Total items:\", total)\n",
    "\n",
    "    for i in range(total):\n",
    "        obj = get_item(i)\n",
    "        print(f\"Item {i}:\", obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd50ee3",
   "metadata": {},
   "source": [
    "# Memmap Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f04aed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside NPZ: x.npy compressed? False\n",
      "Loaded array type: <class 'numpy.ndarray'>\n",
      "Array contents: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "filename = \"test_data.npz\"\n",
    "\n",
    "# Clean up any old file\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "\n",
    "# 1. Create some sample data\n",
    "arr = np.arange(10, dtype=np.int64)\n",
    "\n",
    "# 2. Save it as an uncompressed npz\n",
    "np.savez(filename, x=arr)\n",
    "\n",
    "# 3. Check the compression type within the npz file\n",
    "#    We expect 'ZIP_STORED' (i.e. compression type == 0)\n",
    "with zipfile.ZipFile(filename, \"r\") as zf:\n",
    "    for info in zf.infolist():\n",
    "        print(f\"Inside NPZ: {info.filename} compressed?\", info.compress_type != 0)\n",
    "\n",
    "# 4. Load with mmap_mode\n",
    "#    NOTE: we keep a reference to the returned object to keep the file handle open\n",
    "with np.load(filename, mmap_mode=\"r\") as npzfile:\n",
    "    mmapped_arr = npzfile[\"x\"]\n",
    "    print(\"Loaded array type:\", type(mmapped_arr))\n",
    "    print(\"Array contents:\", mmapped_arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a219e",
   "metadata": {},
   "source": [
    "### Old Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Generic, TypeVar, Optional\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "TGameState = TypeVar(\"TGameState\")  # pylint: disable=invalid-name\n",
    "TAction = TypeVar(\"TAction\")  # pylint: disable=invalid-name\n",
    "TArchiveState = TypeVar(\"TArchiveState\")  # pylint: disable=invalid-name\n",
    "TArchiveAction = TypeVar(\"TArchiveAction\")  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Count21State:\n",
    "    score: int\n",
    "    current_player: int\n",
    "\n",
    "Count21Action = int\n",
    "\n",
    "@dataclass\n",
    "class Connect4State:\n",
    "    board: NDArray[np.int8]  # (height, width)\n",
    "    current_player: int\n",
    "    winner: Optional[int] = None  # The winner, if the game has ended\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Count21ArchiveState:\n",
    "    score: NDArray[np.int64]\n",
    "    current_player: NDArray[np.int8]\n",
    "\n",
    "Count21ArchiveAction = NDArray[np.int8]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Archive(Generic[TGameState, TAction, TArchiveState, TArchiveAction]):\n",
    "    pass\n",
    "\n",
    "print('done.')\n",
    "\n",
    "a = Archive[Count21State, Count21Action, Count21ArchiveState, Count21ArchiveAction]()\n",
    "\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "class Batch(Generic[T]):\n",
    "    \"\"\"Convenience class to convert a sequence of states & actions into a batch.\n",
    "\n",
    "    >>> from dataclasses import dataclass\n",
    "    >>> import torch\n",
    "    >>> @dataclass\n",
    "    ... class GameState:\n",
    "    ...     score: int\n",
    "    ...     current_player: int\n",
    "    >>> @dataclass\n",
    "    ... class BatchGameState(Batch[GameState]):\n",
    "    ...     score: torch.Tensor\n",
    "    ...     current_player: torch.Tensor\n",
    "    >>> states = [GameState(5, 1), GameState(7, 2)]\n",
    "    >>> batch = BatchGameState.from_sequence(states)\n",
    "    >>> len(batch)\n",
    "    2\n",
    "    >>> batch\n",
    "    BatchGameState(score=tensor([5, 7]), current_player=tensor([1, 2]))\n",
    "    >>> batch[0]\n",
    "    GameState(score=5, current_player=1)\n",
    "    \"\"\"\n",
    "\n",
    "    _unbatch_class: Type[T]\n",
    "\n",
    "    @classmethod\n",
    "    def from_sequence(cls: Type[TBatch], items: Sequence[T]) -> TBatch:\n",
    "        if not items:\n",
    "            raise ValueError(\"Cannot create a batch from an empty sequence\")\n",
    "\n",
    "        cls_fields = set(f.name for f in fields(cls))  # type: ignore\n",
    "        batch_dict = {}\n",
    "        for field in fields(items[0]):  # type: ignore\n",
    "            if field.name not in cls_fields:\n",
    "                continue\n",
    "            values = [getattr(item, field.name) for item in items]\n",
    "            # We need to handle both primitive values and torch.Tensors here.\n",
    "            # torch.tensor(primitive_list) is probably more efficient, but doesn't work for tensors.\n",
    "            batch_dict[field.name] = torch.stack([torch.tensor(value) for value in values])\n",
    "\n",
    "        batch = cls(**batch_dict)\n",
    "        batch._unbatch_class = type(items[0])\n",
    "        return batch\n",
    "\n",
    "    def __getitem__(self, index: int) -> T:\n",
    "        item_dict = {field.name: field.type(getattr(self, field.name)[index]) for field in fields(self)}  # type: ignore\n",
    "        return self._unbatch_class(**item_dict)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(getattr(self, fields(self)[0].name))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batchable(Protocol[T]):\n",
    "    \"\"\"Protocol to convert single states & actions into torch.Tensor for batching.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def from_sequence(items: Sequence[T]) -> \"Batchable[T]\": ...\n",
    "\n",
    "    def __getitem__(self, index: int) -> T: ...\n",
    "\n",
    "    def __len__(self) -> int: ...\n",
    "\n",
    "\n",
    "class Batch(Generic[T]):\n",
    "    \"\"\"Convenience class to convert a sequence of states & actions into a batch.\n",
    "\n",
    "    >>> from dataclasses import dataclass\n",
    "    >>> import torch\n",
    "    >>> @dataclass\n",
    "    ... class GameState:\n",
    "    ...     score: int\n",
    "    ...     current_player: int\n",
    "    >>> @dataclass\n",
    "    ... class BatchGameState(Batch[GameState]):\n",
    "    ...     score: torch.Tensor\n",
    "    ...     current_player: torch.Tensor\n",
    "    >>> states = [GameState(5, 1), GameState(7, 2)]\n",
    "    >>> batch = BatchGameState.from_sequence(states)\n",
    "    >>> len(batch)\n",
    "    2\n",
    "    >>> batch\n",
    "    BatchGameState(score=tensor([5, 7]), current_player=tensor([1, 2]))\n",
    "    >>> batch[0]\n",
    "    GameState(score=5, current_player=1)\n",
    "    \"\"\"\n",
    "\n",
    "    _unbatch_class: Type[T]\n",
    "\n",
    "    @classmethod\n",
    "    def from_sequence(cls: Type[TBatch], items: Sequence[T]) -> TBatch:\n",
    "        if not items:\n",
    "            raise ValueError(\"Cannot create a batch from an empty sequence\")\n",
    "\n",
    "        cls_fields = set(f.name for f in fields(cls))  # type: ignore\n",
    "        batch_dict = {}\n",
    "        for field in fields(items[0]):  # type: ignore\n",
    "            if field.name not in cls_fields:\n",
    "                continue\n",
    "            values = [getattr(item, field.name) for item in items]\n",
    "            # We need to handle both primitive values and torch.Tensors here.\n",
    "            # torch.tensor(primitive_list) is probably more efficient, but doesn't work for tensors.\n",
    "            batch_dict[field.name] = torch.stack([torch.tensor(value) for value in values])\n",
    "\n",
    "        batch = cls(**batch_dict)\n",
    "        batch._unbatch_class = type(items[0])\n",
    "        return batch\n",
    "\n",
    "    def __getitem__(self, index: int) -> T:\n",
    "        item_dict = {field.name: field.type(getattr(self, field.name)[index]) for field in fields(self)}  # type: ignore\n",
    "        return self._unbatch_class(**item_dict)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(getattr(self, fields(self)[0].name))  # type: ignore\n",
    "\n",
    "@dataclass\n",
    "class PrimitiveBatch(Generic[T]):\n",
    "    \"\"\"A batch class for primitive types like int, float, etc.\n",
    "\n",
    "    >>> batch = PrimitiveBatch.from_sequence([2,4,6,8])\n",
    "    >>> len(batch)\n",
    "    4\n",
    "    >>> batch\n",
    "    PrimitiveBatch(values=tensor([2, 4, 6, 8]))\n",
    "    >>> batch[0]\n",
    "    2\n",
    "    \"\"\"\n",
    "\n",
    "    values: torch.Tensor\n",
    "\n",
    "    @classmethod\n",
    "    def from_sequence(cls: Type[\"PrimitiveBatch[T]\"], items: Sequence[T]) -> \"PrimitiveBatch[T]\":\n",
    "        if not items:\n",
    "            raise ValueError(\"Cannot create a batch from an empty sequence\")\n",
    "\n",
    "        return cls(values=torch.tensor(items))\n",
    "\n",
    "    def __getitem__(self, index: int) -> T:\n",
    "        return self.values[index].item()  # type: ignore\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.values.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Generic\n",
    "\n",
    "TGameState = TypeVar('TGameState')\n",
    "TArchiveState = TypeVar('TArchiveState')\n",
    "\n",
    "class Archive(Generic[TGameState, TArchiveState]):\n",
    "    def __init__(self):\n",
    "        # Access the original class with type parameters\n",
    "        orig_class = self.__orig_class__\n",
    "        # Extract the type arguments\n",
    "        t_game_state, t_archive_state = orig_class.__args__\n",
    "        print(f\"TGameState type: {t_game_state}\")\n",
    "        print(f\"TArchiveState type: {t_archive_state}\")\n",
    "\n",
    "# Example classes\n",
    "class Foo:\n",
    "    pass\n",
    "\n",
    "class Bar:\n",
    "    pass\n",
    "\n",
    "# Create an instance with specific types\n",
    "a = Archive[Foo, Bar]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Type, TypeVar, Generic\n",
    "\n",
    "TGameState = TypeVar('TGameState')\n",
    "TArchiveState = TypeVar('TArchiveState')\n",
    "\n",
    "@dataclass\n",
    "class Archive(Generic[TGameState, TArchiveState]):\n",
    "    _game_state_type: Type[TGameState] = field(init=False, repr=False)\n",
    "    _archive_state_type: Type[TArchiveState] = field(init=False, repr=False)\n",
    "\n",
    "    def __init__(self, game_state_type: Type[TGameState], archive_state_type: Type[TArchiveState]):\n",
    "        self._game_state_type = game_state_type\n",
    "        self._archive_state_type = archive_state_type\n",
    "        print(f\"TGameState type: {self._game_state_type}\")\n",
    "        print(f\"TArchiveState type: {self._archive_state_type}\")\n",
    "\n",
    "# Define Foo and Bar as example dataclasses\n",
    "@dataclass\n",
    "class Foo:\n",
    "    name: str = \"Foo example\"\n",
    "\n",
    "@dataclass\n",
    "class Bar:\n",
    "    value: int = 42\n",
    "\n",
    "# Instantiate Archive by passing the types\n",
    "a = Archive(Foo, Bar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Type, TypeVar, Generic\n",
    "\n",
    "TGameState = TypeVar('TGameState')\n",
    "TArchiveState = TypeVar('TArchiveState')\n",
    "\n",
    "class Archive(Generic[TGameState, TArchiveState]):\n",
    "\n",
    "    def __init__(self, game_state_type: Type[TGameState], archive_state_type: Type[TArchiveState]):\n",
    "        self._game_state_type = game_state_type\n",
    "        self._archive_state_type = archive_state_type\n",
    "        print(f\"TGameState type: {self._game_state_type}\")\n",
    "        print(f\"TArchiveState type: {self._archive_state_type}\")\n",
    "\n",
    "# Define Foo and Bar as example dataclasses\n",
    "@dataclass\n",
    "class Foo:\n",
    "    name: str = \"Foo example\"\n",
    "\n",
    "@dataclass\n",
    "class Bar:\n",
    "    value: int = 42\n",
    "\n",
    "# Instantiate Archive by passing the types\n",
    "a = Archive(Foo, Bar)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
